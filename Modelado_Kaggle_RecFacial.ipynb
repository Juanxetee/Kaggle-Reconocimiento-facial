{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción y Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Configuración warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar y procesar imágenes de entrenamiento\n",
    "def load_and_preprocess_images(data, img_size=(48, 48), path_col='path', label_col='label', filter_labels=None):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for index, row in data.iterrows():\n",
    "        if filter_labels is None or row[label_col] in filter_labels:\n",
    "            img_path = row[path_col].replace('..', 'data').replace('\\\\', '/')\n",
    "            img = load_img(img_path, target_size=img_size, color_mode='grayscale')\n",
    "            img_array = img_to_array(img)\n",
    "            images.append(img_array)\n",
    "            labels.append(row[label_col])\n",
    "    images = np.array(images, dtype='float32') / 255.0\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar y procesar imágenes sin etiquetas (para el conjunto de prueba)\n",
    "def load_and_preprocess_images2(data, img_size=(48, 48)):\n",
    "    images = []\n",
    "    for index, row in data.iterrows():\n",
    "        img_path = f\"data/data/images/test/{row['id_img']}.jpg\"\n",
    "        img = load_img(img_path, target_size=img_size, color_mode='grayscale')\n",
    "        img_array = img_to_array(img)\n",
    "        images.append(img_array)\n",
    "    images = np.array(images, dtype='float32') / 255.0\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "train_data = pd.read_csv('data/train_set.csv')\n",
    "test_data = pd.read_csv('test_set.csv')\n",
    "sample_submission = pd.read_csv('sample_submision.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar los nombres de las columnas según sea necesario\n",
    "path_col = 'path'  # Cambia esto si el nombre de la columna es diferente\n",
    "label_col = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas de train_set.csv: Index(['id_img', 'path', 'label'], dtype='object')\n",
      "Columnas de test_set.csv: Index(['id_img'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las columnas de los DataFrames\n",
    "print(\"Columnas de train_set.csv:\", train_data.columns)\n",
    "print(\"Columnas de test_set.csv:\", test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y procesar imágenes de entrenamiento\n",
    "train_images, train_labels = load_and_preprocess_images(train_data, path_col=path_col, label_col=label_col, filter_labels=['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "train_labels_categorical = to_categorical(train_labels_encoded, num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjunto de entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels_categorical, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la red neuronal\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m  1/303\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:34\u001b[0m 2s/step - accuracy: 0.8438 - loss: 0.4460"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\buque\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 50ms/step - accuracy: 0.8168 - loss: 0.3977 - val_accuracy: 0.8620 - val_loss: 0.3155\n",
      "Epoch 2/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8310 - loss: 0.3638 - val_accuracy: 0.8563 - val_loss: 0.3275\n",
      "Epoch 3/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.8341 - loss: 0.3652 - val_accuracy: 0.8686 - val_loss: 0.3080\n",
      "Epoch 4/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8431 - loss: 0.3512 - val_accuracy: 0.8604 - val_loss: 0.3109\n",
      "Epoch 5/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 44ms/step - accuracy: 0.8451 - loss: 0.3334 - val_accuracy: 0.8658 - val_loss: 0.3236\n",
      "Epoch 6/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - accuracy: 0.8553 - loss: 0.3184 - val_accuracy: 0.8645 - val_loss: 0.3051\n",
      "Epoch 7/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - accuracy: 0.8621 - loss: 0.3213 - val_accuracy: 0.8666 - val_loss: 0.3155\n",
      "Epoch 8/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.8592 - loss: 0.3265 - val_accuracy: 0.8695 - val_loss: 0.2948\n",
      "Epoch 9/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8665 - loss: 0.3161 - val_accuracy: 0.8757 - val_loss: 0.2928\n",
      "Epoch 10/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8607 - loss: 0.3049 - val_accuracy: 0.8794 - val_loss: 0.2814\n",
      "Epoch 11/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - accuracy: 0.8642 - loss: 0.3052 - val_accuracy: 0.8740 - val_loss: 0.3035\n",
      "Epoch 12/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - accuracy: 0.8728 - loss: 0.3053 - val_accuracy: 0.8724 - val_loss: 0.3032\n",
      "Epoch 13/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.8769 - loss: 0.2835 - val_accuracy: 0.8781 - val_loss: 0.2887\n",
      "Epoch 14/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 44ms/step - accuracy: 0.8861 - loss: 0.2709 - val_accuracy: 0.8686 - val_loss: 0.3107\n",
      "Epoch 15/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8737 - loss: 0.2831 - val_accuracy: 0.8769 - val_loss: 0.2842\n",
      "Epoch 16/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - accuracy: 0.8732 - loss: 0.2827 - val_accuracy: 0.8765 - val_loss: 0.3036\n",
      "Epoch 17/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - accuracy: 0.8839 - loss: 0.2676 - val_accuracy: 0.8773 - val_loss: 0.2884\n",
      "Epoch 18/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8910 - loss: 0.2687 - val_accuracy: 0.8711 - val_loss: 0.3119\n",
      "Epoch 19/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 44ms/step - accuracy: 0.8829 - loss: 0.2769 - val_accuracy: 0.8864 - val_loss: 0.2833\n",
      "Epoch 20/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - accuracy: 0.8907 - loss: 0.2573 - val_accuracy: 0.8711 - val_loss: 0.3028\n",
      "Epoch 21/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.8821 - loss: 0.2669 - val_accuracy: 0.8881 - val_loss: 0.2785\n",
      "Epoch 22/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - accuracy: 0.8934 - loss: 0.2517 - val_accuracy: 0.8761 - val_loss: 0.2824\n",
      "Epoch 23/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 44ms/step - accuracy: 0.8953 - loss: 0.2572 - val_accuracy: 0.8856 - val_loss: 0.2793\n",
      "Epoch 24/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 44ms/step - accuracy: 0.8905 - loss: 0.2615 - val_accuracy: 0.8732 - val_loss: 0.2957\n",
      "Epoch 25/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8932 - loss: 0.2528 - val_accuracy: 0.8753 - val_loss: 0.3048\n",
      "Epoch 26/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.9037 - loss: 0.2478 - val_accuracy: 0.8893 - val_loss: 0.2714\n",
      "Epoch 27/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - accuracy: 0.8943 - loss: 0.2473 - val_accuracy: 0.8856 - val_loss: 0.2781\n",
      "Epoch 28/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 52ms/step - accuracy: 0.9004 - loss: 0.2376 - val_accuracy: 0.8881 - val_loss: 0.2762\n",
      "Epoch 29/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - accuracy: 0.9022 - loss: 0.2379 - val_accuracy: 0.9013 - val_loss: 0.2606\n",
      "Epoch 30/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.8972 - loss: 0.2426 - val_accuracy: 0.8905 - val_loss: 0.2664\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=32), epochs=30, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8961 - loss: 0.2568\n",
      "Validation accuracy: 0.8905\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y procesar imágenes de prueba\n",
    "test_images = load_and_preprocess_images2(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de que las imágenes de prueba tengan la forma correcta\n",
    "if test_images.ndim == 3:\n",
    "    test_images = np.expand_dims(test_images, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar la forma de las imágenes de prueba\n",
    "print(f\"Forma de las imágenes de prueba: {test_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de que no hay imágenes con dimensiones incorrectas\n",
    "for i, img in enumerate(test_images):\n",
    "    if img.shape != (48, 48, 1):\n",
    "        print(f\"Imagen en índice {i} tiene una forma incorrecta: {img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step\n"
     ]
    }
   ],
   "source": [
    "# Realizar predicciones sobre el conjunto de prueba\n",
    "predictions = model.predict(test_images)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_labels_decoded = label_encoder.inverse_transform(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar el archivo de submission\n",
    "submission = sample_submission.copy()\n",
    "submission['label'] = predicted_labels_decoded\n",
    "submission.to_csv('submission_deeper_cnn,epochs20,batch_size32.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_img</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10052</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10065</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10079</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10095</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10121</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7061</th>\n",
       "      <td>9806</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7062</th>\n",
       "      <td>9830</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7063</th>\n",
       "      <td>9853</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7064</th>\n",
       "      <td>9878</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7065</th>\n",
       "      <td>993</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7066 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_img  label\n",
       "0      10052  happy\n",
       "1      10065    sad\n",
       "2      10079    sad\n",
       "3      10095    sad\n",
       "4      10121    sad\n",
       "...      ...    ...\n",
       "7061    9806    sad\n",
       "7062    9830  happy\n",
       "7063    9853    sad\n",
       "7064    9878  happy\n",
       "7065     993    sad\n",
       "\n",
       "[7066 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_type='simple_cnn'):\n",
    "    if model_type == 'simple_cnn':\n",
    "        model = Sequential([\n",
    "            Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(64, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(128, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Flatten(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(7, activation='softmax')\n",
    "        ])\n",
    "    elif model_type == 'deeper_cnn':\n",
    "        model = Sequential([\n",
    "            Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "            Conv2D(32, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(64, (3, 3), activation='relu'),\n",
    "            Conv2D(64, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(128, (3, 3), activation='relu'),\n",
    "            Conv2D(128, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Flatten(),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(7, activation='softmax')\n",
    "        ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels_categorical, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo: simple_cnn, epochs: 10, batch_size: 32\n",
      "Epoch 1/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - accuracy: 0.2505 - loss: 1.8207 - val_accuracy: 0.3759 - val_loss: 1.6038\n",
      "Epoch 2/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 39ms/step - accuracy: 0.3729 - loss: 1.5987 - val_accuracy: 0.4342 - val_loss: 1.4337\n",
      "Epoch 3/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 39ms/step - accuracy: 0.4316 - loss: 1.4604 - val_accuracy: 0.4767 - val_loss: 1.3836\n",
      "Epoch 4/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 39ms/step - accuracy: 0.4674 - loss: 1.3857 - val_accuracy: 0.4905 - val_loss: 1.3132\n",
      "Epoch 5/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 39ms/step - accuracy: 0.4889 - loss: 1.3272 - val_accuracy: 0.5176 - val_loss: 1.2759\n",
      "Epoch 6/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 40ms/step - accuracy: 0.5133 - loss: 1.2760 - val_accuracy: 0.5226 - val_loss: 1.2577\n",
      "Epoch 7/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 40ms/step - accuracy: 0.5272 - loss: 1.2385 - val_accuracy: 0.5258 - val_loss: 1.2525\n",
      "Epoch 8/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 39ms/step - accuracy: 0.5466 - loss: 1.1876 - val_accuracy: 0.5322 - val_loss: 1.2199\n",
      "Epoch 9/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 42ms/step - accuracy: 0.5656 - loss: 1.1354 - val_accuracy: 0.5294 - val_loss: 1.2335\n",
      "Epoch 10/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 40ms/step - accuracy: 0.5731 - loss: 1.1059 - val_accuracy: 0.5402 - val_loss: 1.2142\n",
      "Validation accuracy: 0.5402\n",
      "Entrenando modelo: simple_cnn, epochs: 10, batch_size: 64\n",
      "Epoch 1/10\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 100ms/step - accuracy: 0.2451 - loss: 1.8165 - val_accuracy: 0.3873 - val_loss: 1.5647\n",
      "Epoch 2/10\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 104ms/step - accuracy: 0.3852 - loss: 1.5827 - val_accuracy: 0.4500 - val_loss: 1.4244\n",
      "Epoch 3/10\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 85ms/step - accuracy: 0.4478 - loss: 1.4346 - val_accuracy: 0.4751 - val_loss: 1.3830\n",
      "Validation accuracy: 0.3873\n",
      "Entrenando modelo: simple_cnn, epochs: 20, batch_size: 32\n",
      "Epoch 1/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - accuracy: 0.2445 - loss: 1.8262 - val_accuracy: 0.4000 - val_loss: 1.5917\n",
      "Epoch 2/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 44ms/step - accuracy: 0.3864 - loss: 1.5640 - val_accuracy: 0.4604 - val_loss: 1.4231\n",
      "Epoch 3/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 44ms/step - accuracy: 0.4510 - loss: 1.4230 - val_accuracy: 0.4869 - val_loss: 1.3648\n",
      "Validation accuracy: 0.4000\n",
      "Entrenando modelo: simple_cnn, epochs: 20, batch_size: 64\n",
      "Epoch 1/20\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 93ms/step - accuracy: 0.2351 - loss: 1.8266 - val_accuracy: 0.3941 - val_loss: 1.5510\n",
      "Epoch 2/20\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 84ms/step - accuracy: 0.3899 - loss: 1.5560 - val_accuracy: 0.4422 - val_loss: 1.4502\n",
      "Epoch 3/20\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 87ms/step - accuracy: 0.4452 - loss: 1.4358 - val_accuracy: 0.4892 - val_loss: 1.3444\n",
      "Validation accuracy: 0.3941\n",
      "Entrenando modelo: deeper_cnn, epochs: 10, batch_size: 32\n",
      "Epoch 1/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 103ms/step - accuracy: 0.2354 - loss: 1.8395 - val_accuracy: 0.2614 - val_loss: 1.8084\n",
      "Epoch 2/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 106ms/step - accuracy: 0.2456 - loss: 1.8183 - val_accuracy: 0.2614 - val_loss: 1.8074\n",
      "Epoch 3/10\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 109ms/step - accuracy: 0.2472 - loss: 1.8168 - val_accuracy: 0.2614 - val_loss: 1.8062\n",
      "Validation accuracy: 0.2614\n",
      "Entrenando modelo: deeper_cnn, epochs: 10, batch_size: 64\n",
      "Epoch 1/10\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 188ms/step - accuracy: 0.2319 - loss: 1.8398 - val_accuracy: 0.2614 - val_loss: 1.8087\n",
      "Epoch 2/10\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 184ms/step - accuracy: 0.2455 - loss: 1.8133 - val_accuracy: 0.3122 - val_loss: 1.7253\n",
      "Epoch 3/10\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 186ms/step - accuracy: 0.3284 - loss: 1.6673 - val_accuracy: 0.3839 - val_loss: 1.5657\n",
      "Validation accuracy: 0.2614\n",
      "Entrenando modelo: deeper_cnn, epochs: 20, batch_size: 32\n",
      "Epoch 1/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 103ms/step - accuracy: 0.2319 - loss: 1.8399 - val_accuracy: 0.2614 - val_loss: 1.8083\n",
      "Epoch 2/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 98ms/step - accuracy: 0.2479 - loss: 1.8175 - val_accuracy: 0.2614 - val_loss: 1.8084\n",
      "Epoch 3/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 98ms/step - accuracy: 0.2464 - loss: 1.8138 - val_accuracy: 0.2614 - val_loss: 1.8069\n",
      "Validation accuracy: 0.2614\n",
      "Entrenando modelo: deeper_cnn, epochs: 20, batch_size: 64\n",
      "Epoch 1/20\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 182ms/step - accuracy: 0.2344 - loss: 1.8373 - val_accuracy: 0.2683 - val_loss: 1.7875\n",
      "Epoch 2/20\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 182ms/step - accuracy: 0.2836 - loss: 1.7448 - val_accuracy: 0.3820 - val_loss: 1.5756\n",
      "Epoch 3/20\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 180ms/step - accuracy: 0.3988 - loss: 1.5455 - val_accuracy: 0.4562 - val_loss: 1.4014\n",
      "Validation accuracy: 0.2683\n",
      "Mejores Parámetros: {'model_type': 'simple_cnn', 'epochs': 10, 'batch_size': 32}\n",
      "Mejor Score: 0.5401561260223389\n"
     ]
    }
   ],
   "source": [
    "# Parámetros para probar\n",
    "model_types = ['simple_cnn', 'deeper_cnn']\n",
    "epochs_list = [10, 20]\n",
    "batch_sizes = [32, 64]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "# Entrenar y evaluar cada combinación de parámetros\n",
    "for model_type in model_types:\n",
    "    for epochs in epochs_list:\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"Entrenando modelo: {model_type}, epochs: {epochs}, batch_size: {batch_size}\")\n",
    "            model = create_model(model_type=model_type)\n",
    "            model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "            model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "            val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "                best_params = {'model_type': model_type, 'epochs': epochs, 'batch_size': batch_size}\n",
    "                best_model = model\n",
    "\n",
    "# Imprimir los mejores parámetros y el mejor score\n",
    "print(f\"Mejores Parámetros: {best_params}\")\n",
    "print(f\"Mejor Score: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step\n"
     ]
    }
   ],
   "source": [
    "# Realizar predicciones sobre el conjunto de prueba\n",
    "predictions = best_model.predict(test_images)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_labels_decoded = label_encoder.inverse_transform(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar el archivo de submission con el nombre del modelo\n",
    "submission_name = f\"submission_{best_params['model_type']}.csv\"\n",
    "submission = sample_submission.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as submission_simple_cnn.csv\n"
     ]
    }
   ],
   "source": [
    "# Asegurar que el submission contenga las columnas para las 7 emociones\n",
    "submission['label'] = predicted_labels_decoded\n",
    "\n",
    "submission.to_csv(f'data/submission/{submission_name}', index=False)\n",
    "\n",
    "print(f\"Submission file saved as {submission_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id_img     label\n",
      "0      10052   neutral\n",
      "1      10065       sad\n",
      "2      10079     angry\n",
      "3      10095     angry\n",
      "4      10121     angry\n",
      "...      ...       ...\n",
      "7061    9806  surprise\n",
      "7062    9830     happy\n",
      "7063    9853  surprise\n",
      "7064    9878     happy\n",
      "7065     993  surprise\n",
      "\n",
      "[7066 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "happy       1816\n",
       "sad         1533\n",
       "neutral     1492\n",
       "surprise     962\n",
       "angry        857\n",
       "fear         402\n",
       "disgust        4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['label'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

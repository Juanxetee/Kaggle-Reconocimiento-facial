{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción y Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar y procesar imágenes de entrenamiento\n",
    "def load_and_preprocess_images(data, img_size=(48, 48), path_col='path', label_col='label', filter_labels=None):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for index, row in data.iterrows():\n",
    "        if filter_labels is None or row[label_col] in filter_labels:\n",
    "            img_path = row[path_col].replace('..', 'data').replace('\\\\', '/')\n",
    "            img = load_img(img_path, target_size=img_size, color_mode='grayscale')\n",
    "            img_array = img_to_array(img)\n",
    "            images.append(img_array)\n",
    "            labels.append(row[label_col])\n",
    "    images = np.array(images, dtype='float32') / 255.0\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar y procesar imágenes sin etiquetas (para el conjunto de prueba)\n",
    "def load_and_preprocess_images2(data, img_size=(48, 48)):\n",
    "    images = []\n",
    "    for index, row in data.iterrows():\n",
    "        img_path = f\"data/data/images/test/{row['id_img']}.jpg\"\n",
    "        img = load_img(img_path, target_size=img_size, color_mode='grayscale')\n",
    "        img_array = img_to_array(img)\n",
    "        images.append(img_array)\n",
    "    images = np.array(images, dtype='float32') / 255.0\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "train_data = pd.read_csv('data/train_set.csv')\n",
    "test_data = pd.read_csv('test_set.csv')\n",
    "sample_submission = pd.read_csv('sample_submision.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas de train_set.csv: Index(['id_img', 'path', 'label'], dtype='object')\n",
      "Columnas de test_set.csv: Index(['id_img'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las columnas de los DataFrames\n",
    "print(\"Columnas de train_set.csv:\", train_data.columns)\n",
    "print(\"Columnas de test_set.csv:\", test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y procesar imágenes de entrenamiento\n",
    "train_images, train_labels = load_and_preprocess_images(train_data, path_col=path_col, label_col=label_col, filter_labels=['happy', 'sad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Codificar etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "train_labels_categorical = to_categorical(train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjunto de entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels_categorical, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la red neuronal\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m  1/303\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:34\u001b[0m 2s/step - accuracy: 0.8438 - loss: 0.4460"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\buque\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 50ms/step - accuracy: 0.8168 - loss: 0.3977 - val_accuracy: 0.8620 - val_loss: 0.3155\n",
      "Epoch 2/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8310 - loss: 0.3638 - val_accuracy: 0.8563 - val_loss: 0.3275\n",
      "Epoch 3/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.8341 - loss: 0.3652 - val_accuracy: 0.8686 - val_loss: 0.3080\n",
      "Epoch 4/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8431 - loss: 0.3512 - val_accuracy: 0.8604 - val_loss: 0.3109\n",
      "Epoch 5/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 44ms/step - accuracy: 0.8451 - loss: 0.3334 - val_accuracy: 0.8658 - val_loss: 0.3236\n",
      "Epoch 6/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - accuracy: 0.8553 - loss: 0.3184 - val_accuracy: 0.8645 - val_loss: 0.3051\n",
      "Epoch 7/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - accuracy: 0.8621 - loss: 0.3213 - val_accuracy: 0.8666 - val_loss: 0.3155\n",
      "Epoch 8/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.8592 - loss: 0.3265 - val_accuracy: 0.8695 - val_loss: 0.2948\n",
      "Epoch 9/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8665 - loss: 0.3161 - val_accuracy: 0.8757 - val_loss: 0.2928\n",
      "Epoch 10/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8607 - loss: 0.3049 - val_accuracy: 0.8794 - val_loss: 0.2814\n",
      "Epoch 11/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - accuracy: 0.8642 - loss: 0.3052 - val_accuracy: 0.8740 - val_loss: 0.3035\n",
      "Epoch 12/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - accuracy: 0.8728 - loss: 0.3053 - val_accuracy: 0.8724 - val_loss: 0.3032\n",
      "Epoch 13/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.8769 - loss: 0.2835 - val_accuracy: 0.8781 - val_loss: 0.2887\n",
      "Epoch 14/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 44ms/step - accuracy: 0.8861 - loss: 0.2709 - val_accuracy: 0.8686 - val_loss: 0.3107\n",
      "Epoch 15/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8737 - loss: 0.2831 - val_accuracy: 0.8769 - val_loss: 0.2842\n",
      "Epoch 16/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - accuracy: 0.8732 - loss: 0.2827 - val_accuracy: 0.8765 - val_loss: 0.3036\n",
      "Epoch 17/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - accuracy: 0.8839 - loss: 0.2676 - val_accuracy: 0.8773 - val_loss: 0.2884\n",
      "Epoch 18/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8910 - loss: 0.2687 - val_accuracy: 0.8711 - val_loss: 0.3119\n",
      "Epoch 19/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 44ms/step - accuracy: 0.8829 - loss: 0.2769 - val_accuracy: 0.8864 - val_loss: 0.2833\n",
      "Epoch 20/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - accuracy: 0.8907 - loss: 0.2573 - val_accuracy: 0.8711 - val_loss: 0.3028\n",
      "Epoch 21/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.8821 - loss: 0.2669 - val_accuracy: 0.8881 - val_loss: 0.2785\n",
      "Epoch 22/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - accuracy: 0.8934 - loss: 0.2517 - val_accuracy: 0.8761 - val_loss: 0.2824\n",
      "Epoch 23/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 44ms/step - accuracy: 0.8953 - loss: 0.2572 - val_accuracy: 0.8856 - val_loss: 0.2793\n",
      "Epoch 24/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 44ms/step - accuracy: 0.8905 - loss: 0.2615 - val_accuracy: 0.8732 - val_loss: 0.2957\n",
      "Epoch 25/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8932 - loss: 0.2528 - val_accuracy: 0.8753 - val_loss: 0.3048\n",
      "Epoch 26/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.9037 - loss: 0.2478 - val_accuracy: 0.8893 - val_loss: 0.2714\n",
      "Epoch 27/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - accuracy: 0.8943 - loss: 0.2473 - val_accuracy: 0.8856 - val_loss: 0.2781\n",
      "Epoch 28/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 52ms/step - accuracy: 0.9004 - loss: 0.2376 - val_accuracy: 0.8881 - val_loss: 0.2762\n",
      "Epoch 29/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - accuracy: 0.9022 - loss: 0.2379 - val_accuracy: 0.9013 - val_loss: 0.2606\n",
      "Epoch 30/30\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.8972 - loss: 0.2426 - val_accuracy: 0.8905 - val_loss: 0.2664\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=32), epochs=30, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8961 - loss: 0.2568\n",
      "Validation accuracy: 0.8905\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y procesar imágenes de prueba\n",
    "test_images = load_and_preprocess_images2(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de que las imágenes de prueba tengan la forma correcta\n",
    "if test_images.ndim == 3:\n",
    "    test_images = np.expand_dims(test_images, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar la forma de las imágenes de prueba\n",
    "print(f\"Forma de las imágenes de prueba: {test_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de que no hay imágenes con dimensiones incorrectas\n",
    "for i, img in enumerate(test_images):\n",
    "    if img.shape != (48, 48, 1):\n",
    "        print(f\"Imagen en índice {i} tiene una forma incorrecta: {img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step\n"
     ]
    }
   ],
   "source": [
    "# Realizar predicciones sobre el conjunto de prueba\n",
    "predictions = model.predict(test_images)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_labels_decoded = label_encoder.inverse_transform(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar el archivo de submission\n",
    "submission = sample_submission.copy()\n",
    "submission['label'] = predicted_labels_decoded\n",
    "submission.to_csv('submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de las imágenes de prueba: (7066, 48, 48, 1)\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n"
     ]
    }
   ],
   "source": [
    "# Ajustar los nombres de las columnas según sea necesario\n",
    "path_col = 'path'  # Cambia esto si el nombre de la columna es diferente\n",
    "label_col = 'label'  # Cambia esto si el nombre de la columna es diferente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_img</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10052</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10065</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10079</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10095</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10121</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7061</th>\n",
       "      <td>9806</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7062</th>\n",
       "      <td>9830</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7063</th>\n",
       "      <td>9853</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7064</th>\n",
       "      <td>9878</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7065</th>\n",
       "      <td>993</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7066 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_img  label\n",
       "0      10052  happy\n",
       "1      10065    sad\n",
       "2      10079    sad\n",
       "3      10095    sad\n",
       "4      10121    sad\n",
       "...      ...    ...\n",
       "7061    9806    sad\n",
       "7062    9830  happy\n",
       "7063    9853    sad\n",
       "7064    9878  happy\n",
       "7065     993  happy\n",
       "\n",
       "[7066 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulación de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import zipfile\n",
    "import Funciones_Kaggle_ReconocimientoFacial as fc\n",
    "\n",
    "# Carga y preprocesamiento de imágenes\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Construcción y entrenamiento del modelo\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_img</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>../data/images/train/angry\\0.jpg</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>../data/images/train/angry\\1.jpg</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>../data/images/train/angry\\10.jpg</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002</td>\n",
       "      <td>../data/images/train/angry\\10002.jpg</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10016</td>\n",
       "      <td>../data/images/train/angry\\10016.jpg</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_img                                  path  label\n",
       "0       0      ../data/images/train/angry\\0.jpg  angry\n",
       "1       1      ../data/images/train/angry\\1.jpg  angry\n",
       "2      10     ../data/images/train/angry\\10.jpg  angry\n",
       "3   10002  ../data/images/train/angry\\10002.jpg  angry\n",
       "4   10016  ../data/images/train/angry\\10016.jpg  angry"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Descomprimir archivos\n",
    "with zipfile.ZipFile('data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/')\n",
    "with zipfile.ZipFile('train_set.csv.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/')\n",
    "\n",
    "# Cargar los datos\n",
    "train_data = pd.read_csv('data/train_set.csv')\n",
    "test_data = pd.read_csv('test_set.csv')\n",
    "sample_submission = pd.read_csv('sample_submision.csv')\n",
    "\n",
    "# Mostrar las primeras filas de los datos de entrenamiento\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12102, 48, 48, 1), (12102, 2))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def load_and_preprocess_images(data, img_size=(48, 48)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for index, row in data.iterrows():\n",
    "        if row['label'] in ['happy', 'sad']:  # Filtrar solo 'happy' y 'sad'\n",
    "            img_path = row['path'].replace('..', 'data').replace('\\\\', '/')\n",
    "            img = load_img(img_path, target_size=img_size, color_mode='grayscale')\n",
    "            img_array = img_to_array(img)\n",
    "            images.append(img_array)\n",
    "            labels.append(row['label'])\n",
    "    images = np.array(images, dtype='float32') / 255.0\n",
    "    return images, labels\n",
    "\n",
    "# Cargar y procesar imágenes de entrenamiento\n",
    "train_images, train_labels = load_and_preprocess_images(train_data)\n",
    "\n",
    "# Codificar etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "train_labels_categorical = to_categorical(train_labels_encoded)\n",
    "\n",
    "# Mostrar la forma de las imágenes y etiquetas de entrenamiento\n",
    "train_images.shape, train_labels_categorical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels_categorical, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\buque\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Definir la red neuronal\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 37ms/step - accuracy: 0.6003 - loss: 0.6707 - val_accuracy: 0.7183 - val_loss: 0.5606\n",
      "Epoch 2/10\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.7359 - loss: 0.5228 - val_accuracy: 0.7869 - val_loss: 0.4428\n",
      "Epoch 3/10\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 40ms/step - accuracy: 0.7828 - loss: 0.4497 - val_accuracy: 0.8162 - val_loss: 0.3931\n",
      "Epoch 4/10\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - accuracy: 0.8233 - loss: 0.3780 - val_accuracy: 0.8319 - val_loss: 0.3553\n",
      "Epoch 5/10\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 36ms/step - accuracy: 0.8538 - loss: 0.3320 - val_accuracy: 0.8373 - val_loss: 0.3453\n",
      "Epoch 6/10\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 37ms/step - accuracy: 0.8550 - loss: 0.3242 - val_accuracy: 0.8513 - val_loss: 0.3324\n",
      "Epoch 7/10\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - accuracy: 0.8811 - loss: 0.2800 - val_accuracy: 0.8604 - val_loss: 0.3330\n",
      "Epoch 8/10\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.8950 - loss: 0.2428 - val_accuracy: 0.8567 - val_loss: 0.3291\n",
      "Epoch 9/10\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 37ms/step - accuracy: 0.9047 - loss: 0.2301 - val_accuracy: 0.8608 - val_loss: 0.3349\n",
      "Epoch 10/10\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 40ms/step - accuracy: 0.9192 - loss: 0.2016 - val_accuracy: 0.8567 - val_loss: 0.3536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2aba202e060>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8539 - loss: 0.3543\n",
      "Validation accuracy: 0.8567\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar y procesar imágenes sin etiquetas (para el conjunto de prueba)\n",
    "def load_and_preprocess_images2(data, img_size=(48, 48)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for index, row in data.iterrows():\n",
    "        if row['id_img'] in ['happy', 'sad']:  # Filtrar solo 'happy' y 'sad'\n",
    "            img_path = row['path'].replace('..', 'data').replace('\\\\', '/')\n",
    "            img = load_img(img_path, target_size=img_size, color_mode='grayscale')\n",
    "            img_array = img_to_array(img)\n",
    "            images.append(img_array)\n",
    "            labels.append(row['label'])\n",
    "    images = np.array(images, dtype='float32') / 255.0\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y procesar imágenes de prueba\n",
    "test_images = load_and_preprocess_images2(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de que las imágenes de prueba tengan la forma correcta\n",
    "if test_images.ndim == 3:\n",
    "    test_images = np.expand_dims(test_images, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Conv2D.call().\n\n\u001b[1mNegative dimension size caused by subtracting 3 from 0 for '{{node sequential_1_1/conv2d_3_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](data, sequential_1_1/conv2d_3_1/convolution/ReadVariableOp)' with input shapes: [2,0,1,1], [3,3,1,32].\u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(2, 0, 1, 1), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Realizar predicciones sobre el conjunto de prueba\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m predicted_labels_decoded \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39minverse_transform(predicted_labels)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Conv2D.call().\n\n\u001b[1mNegative dimension size caused by subtracting 3 from 0 for '{{node sequential_1_1/conv2d_3_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](data, sequential_1_1/conv2d_3_1/convolution/ReadVariableOp)' with input shapes: [2,0,1,1], [3,3,1,32].\u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(2, 0, 1, 1), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# Realizar predicciones sobre el conjunto de prueba\n",
    "predictions = model.predict(test_images)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_labels_decoded = label_encoder.inverse_transform(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image data generators\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Conv2D(16, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')  # 2 classes: happy and sad\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=50, \n",
    "                    batch_size=16, \n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(val_generator)\n",
    "val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'id_img': [int(os.path.splitext(f)[0]) for f in test_images]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = []\n",
    "test_ids = []\n",
    "for image_file in test_images:\n",
    "    image_path = os.path.join(test_dir, image_file)\n",
    "    img = preprocess_image(image_path)\n",
    "    if img is not None:\n",
    "        test_images.append(img)\n",
    "        test_ids.append(int(os.path.splitext(image_file)[0]))\n",
    "\n",
    "test_images = np.array(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)\n",
    "predicted_labels = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 'sad', 1: 'happy'}\n",
    "predicted_labels = [label_map[label] for label in predicted_labels]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

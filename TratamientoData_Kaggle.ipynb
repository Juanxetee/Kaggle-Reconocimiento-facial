{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulación de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import zipfile\n",
    "import Funciones_Kaggle_ReconocimientoFacial as fc\n",
    "\n",
    "# Carga y preprocesamiento de imágenes\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Construcción y entrenamiento del modelo\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comenzar descomprimiendo el archivo y luego exploraremos los datos para entender mejor el contenido y la estructura. Luego, procederemos con el preprocesamiento y el entrenamiento del modelo.\n",
    "\n",
    "Descomprimamos el archivo primero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_file_path = 'data.zip'\n",
    "extracted_folder_path = 'data/facial_expressions'\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder_path)\n",
    "\n",
    "extracted_contents = os.listdir(extracted_folder_path)\n",
    "extracted_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo ZIP contiene una carpeta llamada data. Vamos a explorar más a fondo para ver cómo están organizadas las imágenes dentro de esta carpeta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder_path = os.path.join(extracted_folder_path, 'data')\n",
    "data_contents = os.listdir(data_folder_path)\n",
    "data_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de es encontramos una subcarpeta llamada images. Vamos a explorar esta subcarpeta para ver cómo están organizadas las imágenes y obtener una idea del número de imágenes disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['test', 'train'], 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_folder_path = os.path.join(data_folder_path, 'images')\n",
    "images_contents = os.listdir(images_folder_path)\n",
    "num_images = len(images_contents)\n",
    "images_contents[:10], num_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La carpeta images contiene dos subcarpetas: train y test. Exploraremos ambas carpetas, comencemos con Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise'], 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_folder_path = os.path.join(images_folder_path, 'train')\n",
    "train_contents = os.listdir(train_folder_path)\n",
    "num_train_images = len(train_contents)\n",
    "train_contents[:10], num_train_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que nuestro enfoque está en clasificar entre \"feliz\" y \"triste\", utilizaremos únicamente las carpetas happy y sad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7164, 4938)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_train_folder = os.path.join(train_folder_path, 'happy')\n",
    "sad_train_folder = os.path.join(train_folder_path, 'sad')\n",
    "\n",
    "happy_train_images = os.listdir(happy_train_folder)\n",
    "sad_train_images = os.listdir(sad_train_folder)\n",
    "\n",
    "num_happy_train_images = len(happy_train_images)\n",
    "num_sad_train_images = len(sad_train_images)\n",
    "\n",
    "num_happy_train_images, num_sad_train_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La carpeta de entrenamiento contiene 7164 imágenes de personas felices y 4938 imágenes de personas tristes. \n",
    "\n",
    "Ahora exploraremos Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10004.jpg',\n",
       " '10019.jpg',\n",
       " '10023.jpg',\n",
       " '10029.jpg',\n",
       " '1003.jpg',\n",
       " '10031.jpg',\n",
       " '10033.jpg',\n",
       " '10043.jpg',\n",
       " '10044.jpg',\n",
       " '10048.jpg',\n",
       " '10052.jpg',\n",
       " '10053.jpg',\n",
       " '10056.jpg',\n",
       " '10065.jpg',\n",
       " '10068.jpg',\n",
       " '10073.jpg',\n",
       " '10074.jpg',\n",
       " '10079.jpg',\n",
       " '1008.jpg',\n",
       " '10095.jpg',\n",
       " '10096.jpg',\n",
       " '10097.jpg',\n",
       " '10099.jpg',\n",
       " '101.jpg',\n",
       " '10106.jpg',\n",
       " '10114.jpg',\n",
       " '10116.jpg',\n",
       " '10117.jpg',\n",
       " '10118.jpg',\n",
       " '10121.jpg',\n",
       " '10126.jpg',\n",
       " '10134.jpg',\n",
       " '10138.jpg',\n",
       " '10141.jpg',\n",
       " '10148.jpg',\n",
       " '10150.jpg',\n",
       " '10162.jpg',\n",
       " '10163.jpg',\n",
       " '10171.jpg',\n",
       " '10172.jpg',\n",
       " '10176.jpg',\n",
       " '10185.jpg',\n",
       " '10189.jpg',\n",
       " '1020.jpg',\n",
       " '10215.jpg',\n",
       " '10218.jpg',\n",
       " '1022.jpg',\n",
       " '10237.jpg',\n",
       " '1024.jpg',\n",
       " '10246.jpg',\n",
       " '10247.jpg',\n",
       " '10248.jpg',\n",
       " '10252.jpg',\n",
       " '10257.jpg',\n",
       " '10259.jpg',\n",
       " '1026.jpg',\n",
       " '10263.jpg',\n",
       " '10266.jpg',\n",
       " '10267.jpg',\n",
       " '10268.jpg',\n",
       " '1027.jpg',\n",
       " '10273.jpg',\n",
       " '10276.jpg',\n",
       " '10278.jpg',\n",
       " '10286.jpg',\n",
       " '10292.jpg',\n",
       " '10306.jpg',\n",
       " '10312.jpg',\n",
       " '10315.jpg',\n",
       " '10317.jpg',\n",
       " '1033.jpg',\n",
       " '10336.jpg',\n",
       " '10338.jpg',\n",
       " '10339.jpg',\n",
       " '10344.jpg',\n",
       " '10346.jpg',\n",
       " '10352.jpg',\n",
       " '10355.jpg',\n",
       " '10362.jpg',\n",
       " '10365.jpg',\n",
       " '10367.jpg',\n",
       " '10370.jpg',\n",
       " '10371.jpg',\n",
       " '10372.jpg',\n",
       " '10375.jpg',\n",
       " '10382.jpg',\n",
       " '10383.jpg',\n",
       " '10386.jpg',\n",
       " '10399.jpg',\n",
       " '10406.jpg',\n",
       " '10409.jpg',\n",
       " '10415.jpg',\n",
       " '10423.jpg',\n",
       " '10425.jpg',\n",
       " '10426.jpg',\n",
       " '10432.jpg',\n",
       " '10435.jpg',\n",
       " '10437.jpg',\n",
       " '10443.jpg',\n",
       " '10444.jpg',\n",
       " '10445.jpg',\n",
       " '1045.jpg',\n",
       " '10455.jpg',\n",
       " '10456.jpg',\n",
       " '10458.jpg',\n",
       " '10467.jpg',\n",
       " '10468.jpg',\n",
       " '10480.jpg',\n",
       " '10489.jpg',\n",
       " '10493.jpg',\n",
       " '10512.jpg',\n",
       " '10518.jpg',\n",
       " '10528.jpg',\n",
       " '10532.jpg',\n",
       " '10533.jpg',\n",
       " '10534.jpg',\n",
       " '10535.jpg',\n",
       " '10540.jpg',\n",
       " '10545.jpg',\n",
       " '10549.jpg',\n",
       " '10551.jpg',\n",
       " '10552.jpg',\n",
       " '10554.jpg',\n",
       " '1056.jpg',\n",
       " '10561.jpg',\n",
       " '10571.jpg',\n",
       " '1058.jpg',\n",
       " '10583.jpg',\n",
       " '10607.jpg',\n",
       " '10620.jpg',\n",
       " '10622.jpg',\n",
       " '10624.jpg',\n",
       " '10625.jpg',\n",
       " '10634.jpg',\n",
       " '10638.jpg',\n",
       " '10640.jpg',\n",
       " '10644.jpg',\n",
       " '10647.jpg',\n",
       " '10653.jpg',\n",
       " '10655.jpg',\n",
       " '10676.jpg',\n",
       " '10683.jpg',\n",
       " '10686.jpg',\n",
       " '10693.jpg',\n",
       " '10694.jpg',\n",
       " '10699.jpg',\n",
       " '10701.jpg',\n",
       " '10702.jpg',\n",
       " '10703.jpg',\n",
       " '1071.jpg',\n",
       " '10713.jpg',\n",
       " '10715.jpg',\n",
       " '10734.jpg',\n",
       " '1074.jpg',\n",
       " '10741.jpg',\n",
       " '10743.jpg',\n",
       " '10746.jpg',\n",
       " '1075.jpg',\n",
       " '10755.jpg',\n",
       " '10758.jpg',\n",
       " '10768.jpg',\n",
       " '10769.jpg',\n",
       " '10770.jpg',\n",
       " '10773.jpg',\n",
       " '10774.jpg',\n",
       " '1078.jpg',\n",
       " '10787.jpg',\n",
       " '1079.jpg',\n",
       " '10792.jpg',\n",
       " '10797.jpg',\n",
       " '10800.jpg',\n",
       " '1081.jpg',\n",
       " '10811.jpg',\n",
       " '10813.jpg',\n",
       " '10818.jpg',\n",
       " '10820.jpg',\n",
       " '10823.jpg',\n",
       " '10825.jpg',\n",
       " '10832.jpg',\n",
       " '10837.jpg',\n",
       " '10839.jpg',\n",
       " '10841.jpg',\n",
       " '10854.jpg',\n",
       " '10866.jpg',\n",
       " '10871.jpg',\n",
       " '10879.jpg',\n",
       " '10881.jpg',\n",
       " '10902.jpg',\n",
       " '10903.jpg',\n",
       " '10906.jpg',\n",
       " '10909.jpg',\n",
       " '10916.jpg',\n",
       " '10919.jpg',\n",
       " '10926.jpg',\n",
       " '1093.jpg',\n",
       " '10931.jpg',\n",
       " '10933.jpg',\n",
       " '10935.jpg',\n",
       " '1094.jpg',\n",
       " '10944.jpg',\n",
       " '10950.jpg',\n",
       " '10956.jpg',\n",
       " '10968.jpg',\n",
       " '10970.jpg',\n",
       " '10979.jpg',\n",
       " '10992.jpg',\n",
       " '11000.jpg',\n",
       " '11002.jpg',\n",
       " '1101.jpg',\n",
       " '11017.jpg',\n",
       " '11019.jpg',\n",
       " '11022.jpg',\n",
       " '11024.jpg',\n",
       " '11025.jpg',\n",
       " '11026.jpg',\n",
       " '11028.jpg',\n",
       " '11030.jpg',\n",
       " '11031.jpg',\n",
       " '11032.jpg',\n",
       " '11034.jpg',\n",
       " '11036.jpg',\n",
       " '11038.jpg',\n",
       " '11039.jpg',\n",
       " '11043.jpg',\n",
       " '11054.jpg',\n",
       " '11055.jpg',\n",
       " '11057.jpg',\n",
       " '1106.jpg',\n",
       " '11065.jpg',\n",
       " '1107.jpg',\n",
       " '11075.jpg',\n",
       " '11093.jpg',\n",
       " '11096.jpg',\n",
       " '11098.jpg',\n",
       " '11100.jpg',\n",
       " '11104.jpg',\n",
       " '11108.jpg',\n",
       " '1111.jpg',\n",
       " '11112.jpg',\n",
       " '11126.jpg',\n",
       " '11130.jpg',\n",
       " '11137.jpg',\n",
       " '11138.jpg',\n",
       " '11139.jpg',\n",
       " '11140.jpg',\n",
       " '11143.jpg',\n",
       " '11146.jpg',\n",
       " '1115.jpg',\n",
       " '11150.jpg',\n",
       " '11158.jpg',\n",
       " '1116.jpg',\n",
       " '11163.jpg',\n",
       " '11168.jpg',\n",
       " '11169.jpg',\n",
       " '1118.jpg',\n",
       " '11180.jpg',\n",
       " '11186.jpg',\n",
       " '11188.jpg',\n",
       " '11192.jpg',\n",
       " '112.jpg',\n",
       " '11200.jpg',\n",
       " '11208.jpg',\n",
       " '11209.jpg',\n",
       " '1121.jpg',\n",
       " '11213.jpg',\n",
       " '11218.jpg',\n",
       " '11222.jpg',\n",
       " '11233.jpg',\n",
       " '11234.jpg',\n",
       " '11236.jpg',\n",
       " '1124.jpg',\n",
       " '11241.jpg',\n",
       " '11251.jpg',\n",
       " '1126.jpg',\n",
       " '11264.jpg',\n",
       " '11266.jpg',\n",
       " '11269.jpg',\n",
       " '1127.jpg',\n",
       " '11277.jpg',\n",
       " '11278.jpg',\n",
       " '1128.jpg',\n",
       " '11280.jpg',\n",
       " '11284.jpg',\n",
       " '11295.jpg',\n",
       " '1130.jpg',\n",
       " '11302.jpg',\n",
       " '11316.jpg',\n",
       " '11317.jpg',\n",
       " '1133.jpg',\n",
       " '11337.jpg',\n",
       " '11342.jpg',\n",
       " '11345.jpg',\n",
       " '11346.jpg',\n",
       " '11349.jpg',\n",
       " '11353.jpg',\n",
       " '11358.jpg',\n",
       " '11360.jpg',\n",
       " '1137.jpg',\n",
       " '11381.jpg',\n",
       " '11390.jpg',\n",
       " '11392.jpg',\n",
       " '11395.jpg',\n",
       " '11398.jpg',\n",
       " '11400.jpg',\n",
       " '1141.jpg',\n",
       " '11412.jpg',\n",
       " '11414.jpg',\n",
       " '11416.jpg',\n",
       " '11418.jpg',\n",
       " '1142.jpg',\n",
       " '11421.jpg',\n",
       " '11429.jpg',\n",
       " '11448.jpg',\n",
       " '11451.jpg',\n",
       " '11462.jpg',\n",
       " '11466.jpg',\n",
       " '11467.jpg',\n",
       " '11471.jpg',\n",
       " '1148.jpg',\n",
       " '11491.jpg',\n",
       " '115.jpg',\n",
       " '11500.jpg',\n",
       " '11508.jpg',\n",
       " '11516.jpg',\n",
       " '11518.jpg',\n",
       " '1152.jpg',\n",
       " '11523.jpg',\n",
       " '11529.jpg',\n",
       " '11530.jpg',\n",
       " '11536.jpg',\n",
       " '1154.jpg',\n",
       " '11549.jpg',\n",
       " '11552.jpg',\n",
       " '11558.jpg',\n",
       " '11559.jpg',\n",
       " '1156.jpg',\n",
       " '11560.jpg',\n",
       " '11561.jpg',\n",
       " '11573.jpg',\n",
       " '11575.jpg',\n",
       " '11581.jpg',\n",
       " '11582.jpg',\n",
       " '11587.jpg',\n",
       " '11588.jpg',\n",
       " '11589.jpg',\n",
       " '1159.jpg',\n",
       " '11594.jpg',\n",
       " '11601.jpg',\n",
       " '11603.jpg',\n",
       " '11604.jpg',\n",
       " '11610.jpg',\n",
       " '11613.jpg',\n",
       " '11614.jpg',\n",
       " '11626.jpg',\n",
       " '11632.jpg',\n",
       " '11637.jpg',\n",
       " '11639.jpg',\n",
       " '11640.jpg',\n",
       " '11642.jpg',\n",
       " '11646.jpg',\n",
       " '11653.jpg',\n",
       " '11654.jpg',\n",
       " '11658.jpg',\n",
       " '11660.jpg',\n",
       " '11662.jpg',\n",
       " '11670.jpg',\n",
       " '11675.jpg',\n",
       " '11682.jpg',\n",
       " '11686.jpg',\n",
       " '11687.jpg',\n",
       " '11689.jpg',\n",
       " '1169.jpg',\n",
       " '11694.jpg',\n",
       " '11704.jpg',\n",
       " '11706.jpg',\n",
       " '11717.jpg',\n",
       " '11721.jpg',\n",
       " '11722.jpg',\n",
       " '11725.jpg',\n",
       " '11727.jpg',\n",
       " '11729.jpg',\n",
       " '1173.jpg',\n",
       " '11733.jpg',\n",
       " '11738.jpg',\n",
       " '11746.jpg',\n",
       " '11747.jpg',\n",
       " '11754.jpg',\n",
       " '1176.jpg',\n",
       " '11763.jpg',\n",
       " '11766.jpg',\n",
       " '1177.jpg',\n",
       " '11772.jpg',\n",
       " '11774.jpg',\n",
       " '11783.jpg',\n",
       " '11785.jpg',\n",
       " '11786.jpg',\n",
       " '11790.jpg',\n",
       " '11793.jpg',\n",
       " '11794.jpg',\n",
       " '11808.jpg',\n",
       " '11813.jpg',\n",
       " '11814.jpg',\n",
       " '11816.jpg',\n",
       " '11818.jpg',\n",
       " '11819.jpg',\n",
       " '11823.jpg',\n",
       " '11834.jpg',\n",
       " '11835.jpg',\n",
       " '11836.jpg',\n",
       " '11841.jpg',\n",
       " '11842.jpg',\n",
       " '11848.jpg',\n",
       " '11849.jpg',\n",
       " '11850.jpg',\n",
       " '11855.jpg',\n",
       " '11865.jpg',\n",
       " '11867.jpg',\n",
       " '11872.jpg',\n",
       " '11875.jpg',\n",
       " '11876.jpg',\n",
       " '11880.jpg',\n",
       " '11883.jpg',\n",
       " '11885.jpg',\n",
       " '11889.jpg',\n",
       " '1189.jpg',\n",
       " '11891.jpg',\n",
       " '11902.jpg',\n",
       " '11904.jpg',\n",
       " '11909.jpg',\n",
       " '11913.jpg',\n",
       " '11927.jpg',\n",
       " '11928.jpg',\n",
       " '1193.jpg',\n",
       " '11944.jpg',\n",
       " '11949.jpg',\n",
       " '11964.jpg',\n",
       " '11966.jpg',\n",
       " '11968.jpg',\n",
       " '11977.jpg',\n",
       " '11978.jpg',\n",
       " '1198.jpg',\n",
       " '11986.jpg',\n",
       " '11999.jpg',\n",
       " '12011.jpg',\n",
       " '1202.jpg',\n",
       " '12024.jpg',\n",
       " '12040.jpg',\n",
       " '12042.jpg',\n",
       " '1206.jpg',\n",
       " '12063.jpg',\n",
       " '12074.jpg',\n",
       " '12086.jpg',\n",
       " '12090.jpg',\n",
       " '12092.jpg',\n",
       " '12096.jpg',\n",
       " '12098.jpg',\n",
       " '12100.jpg',\n",
       " '12106.jpg',\n",
       " '12107.jpg',\n",
       " '12113.jpg',\n",
       " '12116.jpg',\n",
       " '12117.jpg',\n",
       " '12119.jpg',\n",
       " '12120.jpg',\n",
       " '12125.jpg',\n",
       " '12129.jpg',\n",
       " '12131.jpg',\n",
       " '12134.jpg',\n",
       " '12136.jpg',\n",
       " '12141.jpg',\n",
       " '12144.jpg',\n",
       " '1215.jpg',\n",
       " '12162.jpg',\n",
       " '12171.jpg',\n",
       " '12173.jpg',\n",
       " '12183.jpg',\n",
       " '12190.jpg',\n",
       " '12191.jpg',\n",
       " '1220.jpg',\n",
       " '12202.jpg',\n",
       " '12215.jpg',\n",
       " '1222.jpg',\n",
       " '12230.jpg',\n",
       " '12237.jpg',\n",
       " '12239.jpg',\n",
       " '12246.jpg',\n",
       " '12255.jpg',\n",
       " '12257.jpg',\n",
       " '12259.jpg',\n",
       " '12267.jpg',\n",
       " '1227.jpg',\n",
       " '12272.jpg',\n",
       " '12284.jpg',\n",
       " '12289.jpg',\n",
       " '12307.jpg',\n",
       " '12312.jpg',\n",
       " '12313.jpg',\n",
       " '12318.jpg',\n",
       " '12319.jpg',\n",
       " '12320.jpg',\n",
       " '12321.jpg',\n",
       " '12329.jpg',\n",
       " '12334.jpg',\n",
       " '12338.jpg',\n",
       " '12349.jpg',\n",
       " '1235.jpg',\n",
       " '12350.jpg',\n",
       " '12352.jpg',\n",
       " '12359.jpg',\n",
       " '12361.jpg',\n",
       " '12363.jpg',\n",
       " '12364.jpg',\n",
       " '12366.jpg',\n",
       " '12376.jpg',\n",
       " '12379.jpg',\n",
       " '12381.jpg',\n",
       " '12383.jpg',\n",
       " '1239.jpg',\n",
       " '12407.jpg',\n",
       " '12410.jpg',\n",
       " '12417.jpg',\n",
       " '12423.jpg',\n",
       " '12431.jpg',\n",
       " '12440.jpg',\n",
       " '1246.jpg',\n",
       " '12463.jpg',\n",
       " '12470.jpg',\n",
       " '12472.jpg',\n",
       " '1248.jpg',\n",
       " '12487.jpg',\n",
       " '12489.jpg',\n",
       " '1249.jpg',\n",
       " '12492.jpg',\n",
       " '12496.jpg',\n",
       " '12497.jpg',\n",
       " '12498.jpg',\n",
       " '12500.jpg',\n",
       " '12504.jpg',\n",
       " '12508.jpg',\n",
       " '12512.jpg',\n",
       " '12519.jpg',\n",
       " '12524.jpg',\n",
       " '12527.jpg',\n",
       " '12529.jpg',\n",
       " '12547.jpg',\n",
       " '12551.jpg',\n",
       " '12557.jpg',\n",
       " '12560.jpg',\n",
       " '12567.jpg',\n",
       " '12569.jpg',\n",
       " '12578.jpg',\n",
       " '12581.jpg',\n",
       " '12591.jpg',\n",
       " '12592.jpg',\n",
       " '12594.jpg',\n",
       " '12600.jpg',\n",
       " '12611.jpg',\n",
       " '12621.jpg',\n",
       " '12626.jpg',\n",
       " '12634.jpg',\n",
       " '1264.jpg',\n",
       " '12647.jpg',\n",
       " '12648.jpg',\n",
       " '12659.jpg',\n",
       " '12661.jpg',\n",
       " '12672.jpg',\n",
       " '12674.jpg',\n",
       " '12676.jpg',\n",
       " '12680.jpg',\n",
       " '12684.jpg',\n",
       " '12695.jpg',\n",
       " '12698.jpg',\n",
       " '12702.jpg',\n",
       " '12705.jpg',\n",
       " '12712.jpg',\n",
       " '12723.jpg',\n",
       " '12725.jpg',\n",
       " '12729.jpg',\n",
       " '12733.jpg',\n",
       " '12737.jpg',\n",
       " '12748.jpg',\n",
       " '12753.jpg',\n",
       " '12758.jpg',\n",
       " '12768.jpg',\n",
       " '12772.jpg',\n",
       " '12776.jpg',\n",
       " '12789.jpg',\n",
       " '12791.jpg',\n",
       " '12801.jpg',\n",
       " '12808.jpg',\n",
       " '12809.jpg',\n",
       " '12810.jpg',\n",
       " '12818.jpg',\n",
       " '12827.jpg',\n",
       " '12841.jpg',\n",
       " '12842.jpg',\n",
       " '12851.jpg',\n",
       " '12852.jpg',\n",
       " '12861.jpg',\n",
       " '12869.jpg',\n",
       " '12879.jpg',\n",
       " '12885.jpg',\n",
       " '12891.jpg',\n",
       " '12899.jpg',\n",
       " '12903.jpg',\n",
       " '12904.jpg',\n",
       " '12924.jpg',\n",
       " '12928.jpg',\n",
       " '12933.jpg',\n",
       " '12936.jpg',\n",
       " '12945.jpg',\n",
       " '12948.jpg',\n",
       " '12951.jpg',\n",
       " '12956.jpg',\n",
       " '12957.jpg',\n",
       " '12958.jpg',\n",
       " '12960.jpg',\n",
       " '12973.jpg',\n",
       " '12976.jpg',\n",
       " '1298.jpg',\n",
       " '12986.jpg',\n",
       " '12988.jpg',\n",
       " '1300.jpg',\n",
       " '13000.jpg',\n",
       " '13009.jpg',\n",
       " '13027.jpg',\n",
       " '13029.jpg',\n",
       " '13037.jpg',\n",
       " '13041.jpg',\n",
       " '13047.jpg',\n",
       " '13051.jpg',\n",
       " '13057.jpg',\n",
       " '13059.jpg',\n",
       " '13067.jpg',\n",
       " '13071.jpg',\n",
       " '13073.jpg',\n",
       " '13080.jpg',\n",
       " '1309.jpg',\n",
       " '13091.jpg',\n",
       " '13092.jpg',\n",
       " '131.jpg',\n",
       " '13101.jpg',\n",
       " '13105.jpg',\n",
       " '13108.jpg',\n",
       " '13114.jpg',\n",
       " '13118.jpg',\n",
       " '13121.jpg',\n",
       " '13127.jpg',\n",
       " '1313.jpg',\n",
       " '13139.jpg',\n",
       " '13145.jpg',\n",
       " '13150.jpg',\n",
       " '13151.jpg',\n",
       " '13152.jpg',\n",
       " '1316.jpg',\n",
       " '13161.jpg',\n",
       " '13162.jpg',\n",
       " '13164.jpg',\n",
       " '13167.jpg',\n",
       " '13169.jpg',\n",
       " '13179.jpg',\n",
       " '13180.jpg',\n",
       " '13187.jpg',\n",
       " '13188.jpg',\n",
       " '13203.jpg',\n",
       " '13204.jpg',\n",
       " '13205.jpg',\n",
       " '13212.jpg',\n",
       " '13220.jpg',\n",
       " '13228.jpg',\n",
       " '13230.jpg',\n",
       " '13233.jpg',\n",
       " '13237.jpg',\n",
       " '13242.jpg',\n",
       " '1325.jpg',\n",
       " '13250.jpg',\n",
       " '13252.jpg',\n",
       " '13267.jpg',\n",
       " '13279.jpg',\n",
       " '1328.jpg',\n",
       " '13283.jpg',\n",
       " '13288.jpg',\n",
       " '13293.jpg',\n",
       " '13296.jpg',\n",
       " '13301.jpg',\n",
       " '13306.jpg',\n",
       " '13308.jpg',\n",
       " '13310.jpg',\n",
       " '13317.jpg',\n",
       " '13322.jpg',\n",
       " '13334.jpg',\n",
       " '13337.jpg',\n",
       " '13339.jpg',\n",
       " '1334.jpg',\n",
       " '13348.jpg',\n",
       " '13351.jpg',\n",
       " '13353.jpg',\n",
       " '1336.jpg',\n",
       " '13362.jpg',\n",
       " '13363.jpg',\n",
       " '13366.jpg',\n",
       " '13372.jpg',\n",
       " '13376.jpg',\n",
       " '13380.jpg',\n",
       " '13385.jpg',\n",
       " '1339.jpg',\n",
       " '13394.jpg',\n",
       " '13395.jpg',\n",
       " '1340.jpg',\n",
       " '13405.jpg',\n",
       " '13409.jpg',\n",
       " '1341.jpg',\n",
       " '13411.jpg',\n",
       " '13421.jpg',\n",
       " '13424.jpg',\n",
       " '13427.jpg',\n",
       " '13428.jpg',\n",
       " '1343.jpg',\n",
       " '13430.jpg',\n",
       " '13431.jpg',\n",
       " '13433.jpg',\n",
       " '13441.jpg',\n",
       " '13449.jpg',\n",
       " '13456.jpg',\n",
       " '13460.jpg',\n",
       " '13467.jpg',\n",
       " '13476.jpg',\n",
       " '13482.jpg',\n",
       " '13483.jpg',\n",
       " '13486.jpg',\n",
       " '13488.jpg',\n",
       " '13498.jpg',\n",
       " '135.jpg',\n",
       " '1350.jpg',\n",
       " '13502.jpg',\n",
       " '13505.jpg',\n",
       " '13510.jpg',\n",
       " '13514.jpg',\n",
       " '13518.jpg',\n",
       " '13521.jpg',\n",
       " '13526.jpg',\n",
       " '13528.jpg',\n",
       " '1353.jpg',\n",
       " '13531.jpg',\n",
       " '13533.jpg',\n",
       " '13535.jpg',\n",
       " '13549.jpg',\n",
       " '13561.jpg',\n",
       " '13562.jpg',\n",
       " '13570.jpg',\n",
       " '13575.jpg',\n",
       " '1358.jpg',\n",
       " '13582.jpg',\n",
       " '13584.jpg',\n",
       " '13585.jpg',\n",
       " '13586.jpg',\n",
       " '13587.jpg',\n",
       " '13590.jpg',\n",
       " '13593.jpg',\n",
       " '13596.jpg',\n",
       " '13597.jpg',\n",
       " '13598.jpg',\n",
       " '13599.jpg',\n",
       " '13603.jpg',\n",
       " '13607.jpg',\n",
       " '13615.jpg',\n",
       " '13618.jpg',\n",
       " '13619.jpg',\n",
       " '13622.jpg',\n",
       " '13631.jpg',\n",
       " '13633.jpg',\n",
       " '13638.jpg',\n",
       " '1364.jpg',\n",
       " '13644.jpg',\n",
       " '13652.jpg',\n",
       " '13653.jpg',\n",
       " '13655.jpg',\n",
       " '13664.jpg',\n",
       " '13666.jpg',\n",
       " '1367.jpg',\n",
       " '13670.jpg',\n",
       " '13671.jpg',\n",
       " '13672.jpg',\n",
       " '13683.jpg',\n",
       " '13684.jpg',\n",
       " '13692.jpg',\n",
       " '13694.jpg',\n",
       " '13695.jpg',\n",
       " '13703.jpg',\n",
       " '13706.jpg',\n",
       " '13714.jpg',\n",
       " '13715.jpg',\n",
       " '13726.jpg',\n",
       " '13738.jpg',\n",
       " '13740.jpg',\n",
       " '13757.jpg',\n",
       " '13758.jpg',\n",
       " '1376.jpg',\n",
       " '13761.jpg',\n",
       " '13792.jpg',\n",
       " '13793.jpg',\n",
       " '13798.jpg',\n",
       " '13800.jpg',\n",
       " '13801.jpg',\n",
       " '13802.jpg',\n",
       " '13808.jpg',\n",
       " '1381.jpg',\n",
       " '13812.jpg',\n",
       " '13825.jpg',\n",
       " '13833.jpg',\n",
       " '13834.jpg',\n",
       " '13837.jpg',\n",
       " '13843.jpg',\n",
       " '13847.jpg',\n",
       " '13848.jpg',\n",
       " '13852.jpg',\n",
       " '13855.jpg',\n",
       " '1386.jpg',\n",
       " '13870.jpg',\n",
       " '13874.jpg',\n",
       " '13876.jpg',\n",
       " '13877.jpg',\n",
       " '13888.jpg',\n",
       " '1389.jpg',\n",
       " '13890.jpg',\n",
       " '13897.jpg',\n",
       " '13899.jpg',\n",
       " '139.jpg',\n",
       " '13907.jpg',\n",
       " '13909.jpg',\n",
       " '13911.jpg',\n",
       " '13912.jpg',\n",
       " '13913.jpg',\n",
       " '13921.jpg',\n",
       " '13925.jpg',\n",
       " '13928.jpg',\n",
       " '13933.jpg',\n",
       " '13939.jpg',\n",
       " '13940.jpg',\n",
       " '1395.jpg',\n",
       " '13955.jpg',\n",
       " '13965.jpg',\n",
       " '13968.jpg',\n",
       " '13969.jpg',\n",
       " '13971.jpg',\n",
       " '13978.jpg',\n",
       " '13979.jpg',\n",
       " '13983.jpg',\n",
       " '13991.jpg',\n",
       " '1401.jpg',\n",
       " '14011.jpg',\n",
       " '14022.jpg',\n",
       " '14030.jpg',\n",
       " '14033.jpg',\n",
       " '14037.jpg',\n",
       " '14041.jpg',\n",
       " '14044.jpg',\n",
       " '14047.jpg',\n",
       " '14048.jpg',\n",
       " '14057.jpg',\n",
       " '14058.jpg',\n",
       " '14063.jpg',\n",
       " '1407.jpg',\n",
       " '14070.jpg',\n",
       " '14080.jpg',\n",
       " '14082.jpg',\n",
       " '14091.jpg',\n",
       " '14092.jpg',\n",
       " '141.jpg',\n",
       " '1410.jpg',\n",
       " '14106.jpg',\n",
       " '14108.jpg',\n",
       " '1411.jpg',\n",
       " '14113.jpg',\n",
       " '14114.jpg',\n",
       " '14115.jpg',\n",
       " '14117.jpg',\n",
       " '14118.jpg',\n",
       " '14120.jpg',\n",
       " '14122.jpg',\n",
       " '14127.jpg',\n",
       " '14135.jpg',\n",
       " '14136.jpg',\n",
       " '14139.jpg',\n",
       " '14148.jpg',\n",
       " '14152.jpg',\n",
       " '14154.jpg',\n",
       " '14155.jpg',\n",
       " '14169.jpg',\n",
       " '14170.jpg',\n",
       " '14174.jpg',\n",
       " '14175.jpg',\n",
       " '14185.jpg',\n",
       " '14187.jpg',\n",
       " '14190.jpg',\n",
       " '1420.jpg',\n",
       " '14206.jpg',\n",
       " '14208.jpg',\n",
       " '1422.jpg',\n",
       " '14224.jpg',\n",
       " '14226.jpg',\n",
       " '14236.jpg',\n",
       " '14243.jpg',\n",
       " '14250.jpg',\n",
       " '14268.jpg',\n",
       " '14278.jpg',\n",
       " '14282.jpg',\n",
       " '1429.jpg',\n",
       " '14292.jpg',\n",
       " '14305.jpg',\n",
       " '14306.jpg',\n",
       " '14310.jpg',\n",
       " '14314.jpg',\n",
       " '14315.jpg',\n",
       " '14327.jpg',\n",
       " '14329.jpg',\n",
       " '14331.jpg',\n",
       " '14342.jpg',\n",
       " '14346.jpg',\n",
       " '14348.jpg',\n",
       " '14352.jpg',\n",
       " '14357.jpg',\n",
       " '14360.jpg',\n",
       " '14362.jpg',\n",
       " '14371.jpg',\n",
       " '14375.jpg',\n",
       " '14376.jpg',\n",
       " '14378.jpg',\n",
       " '1438.jpg',\n",
       " '14381.jpg',\n",
       " '14384.jpg',\n",
       " '14388.jpg',\n",
       " '14406.jpg',\n",
       " '1441.jpg',\n",
       " '14419.jpg',\n",
       " '14421.jpg',\n",
       " '14427.jpg',\n",
       " '14436.jpg',\n",
       " '14438.jpg',\n",
       " '14444.jpg',\n",
       " '14448.jpg',\n",
       " '14452.jpg',\n",
       " '14459.jpg',\n",
       " '14464.jpg',\n",
       " '14472.jpg',\n",
       " '14473.jpg',\n",
       " '14474.jpg',\n",
       " '14475.jpg',\n",
       " '14481.jpg',\n",
       " '14484.jpg',\n",
       " '14487.jpg',\n",
       " '1449.jpg',\n",
       " '14494.jpg',\n",
       " '14495.jpg',\n",
       " '14496.jpg',\n",
       " '14498.jpg',\n",
       " '14505.jpg',\n",
       " '14509.jpg',\n",
       " '14513.jpg',\n",
       " '14516.jpg',\n",
       " '14517.jpg',\n",
       " '14518.jpg',\n",
       " '14522.jpg',\n",
       " '14524.jpg',\n",
       " '14529.jpg',\n",
       " '14542.jpg',\n",
       " '14546.jpg',\n",
       " '14550.jpg',\n",
       " '14551.jpg',\n",
       " '14557.jpg',\n",
       " '14561.jpg',\n",
       " '14564.jpg',\n",
       " '1457.jpg',\n",
       " '14572.jpg',\n",
       " '14574.jpg',\n",
       " '14578.jpg',\n",
       " '14586.jpg',\n",
       " '14593.jpg',\n",
       " '1460.jpg',\n",
       " '14605.jpg',\n",
       " '14611.jpg',\n",
       " '14620.jpg',\n",
       " '14637.jpg',\n",
       " '14641.jpg',\n",
       " '14645.jpg',\n",
       " '14647.jpg',\n",
       " '1465.jpg',\n",
       " '14671.jpg',\n",
       " '14682.jpg',\n",
       " '14691.jpg',\n",
       " '14696.jpg',\n",
       " '14697.jpg',\n",
       " '14700.jpg',\n",
       " '14703.jpg',\n",
       " '14706.jpg',\n",
       " '14717.jpg',\n",
       " '14722.jpg',\n",
       " '14729.jpg',\n",
       " '14730.jpg',\n",
       " '1474.jpg',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_folder_path = os.path.join(images_folder_path, 'test')\n",
    "test_contents = os.listdir(test_folder_path)\n",
    "test_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contiene una gran cantidad de imágenes, pero no están organizadas en subcarpetas de emociones. Dado que la clasificación entre \"feliz\" y \"triste\" debe realizarse, será necesario que evaluemos cómo están etiquetadas estas imágenes o si requieren una clasificación manual para el test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para proceder con la construcción de nuestro modelo, primero prepararemos el conjunto de entrenamiento. Realizaremos los siguientes pasos:\n",
    "\n",
    "1. Cargar las imágenes de las carpetas happy y sad.\n",
    "2. Preprocesar las imágenes (normalización, redimensionamiento si es necesario).\n",
    "3. Crear las etiquetas correspondientes para cada categoría (0 para \"sad\" y 1 para \"happy\").\n",
    "4. Entrenar un modelo de red neuronal convolucional (CNN) utilizando estos datos.\n",
    "\n",
    "Vamos a comenzar con la carga y preprocesamiento de las imágenes de entrenamiento, para ello usaré el módulo de Funciones (fc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_train_images, happy_train_labels = fc.load_images_from_folder(happy_train_folder, label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_train_images, sad_train_labels = fc.load_images_from_folder(sad_train_folder, label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (7164,48,48,1) (4938,48,48,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mhappy_train_images\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msad_train_images\u001b[49m)\n\u001b[0;32m      2\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(happy_train_labels \u001b[38;5;241m+\u001b[39m sad_train_labels)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (7164,48,48,1) (4938,48,48,1) "
     ]
    }
   ],
   "source": [
    "images = np.array(happy_train_images + sad_train_images)\n",
    "labels = np.array(happy_train_labels + sad_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((happy_train_images, sad_train_images), axis=0)\n",
    "y = np.concatenate((happy_train_labels, sad_train_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = shuffle(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12102, 48, 48, 1), (12102,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the images for model compatibility\n",
    "X = X.reshape(X.shape[0], 48, 48, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9681, 48, 48, 1) (9681, 2) (2421, 48, 48, 1) (2421, 2)\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to categorical\n",
    "y_train_cat = to_categorical(y_train, num_classes=2)\n",
    "y_val_cat = to_categorical(y_val, num_classes=2)\n",
    "\n",
    "print(X_train.shape, y_train_cat.shape, X_val.shape, y_val_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Construcción y Entrenamiento del Modelo\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el pipeline con diferentes modelos\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=50)),\n",
    "    ('classifier', svm_model)  # placeholder\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'dropout_rate': [0.3, 0.4, 0.5],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [10, 20]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(optimizer='adam', dropout_rate=0.5):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam()\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop()\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_grid_search(X_train, y_train, param_grid, cv=3):\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    \n",
    "    for optimizer in param_grid['optimizer']:\n",
    "        for dropout_rate in param_grid['dropout_rate']:\n",
    "            for batch_size in param_grid['batch_size']:\n",
    "                for epochs in param_grid['epochs']:\n",
    "                    print(f\"Training with optimizer={optimizer}, dropout_rate={dropout_rate}, batch_size={batch_size}, epochs={epochs}\")\n",
    "                    \n",
    "                    model = create_cnn_model(optimizer=optimizer, dropout_rate=dropout_rate)\n",
    "                    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, verbose=1)\n",
    "                    \n",
    "                    val_accuracy = max(history.history['val_accuracy'])\n",
    "                    if val_accuracy > best_score:\n",
    "                        best_score = val_accuracy\n",
    "                        best_params = {\n",
    "                            'optimizer': optimizer,\n",
    "                            'dropout_rate': dropout_rate,\n",
    "                            'batch_size': batch_size,\n",
    "                            'epochs': epochs\n",
    "                        }\n",
    "    \n",
    "    return best_score, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with optimizer=adam, dropout_rate=0.3, batch_size=32, epochs=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\buque\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - accuracy: 0.5802 - loss: 0.6811 - val_accuracy: 0.6242 - val_loss: 0.6604\n",
      "Epoch 2/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.6311 - loss: 0.6391 - val_accuracy: 0.6892 - val_loss: 0.5685\n",
      "Epoch 3/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 0.7139 - loss: 0.5547 - val_accuracy: 0.7656 - val_loss: 0.4862\n",
      "Epoch 4/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.7433 - loss: 0.5031 - val_accuracy: 0.7723 - val_loss: 0.4694\n",
      "Epoch 5/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - accuracy: 0.7752 - loss: 0.4621 - val_accuracy: 0.8090 - val_loss: 0.4130\n",
      "Epoch 6/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.7913 - loss: 0.4381 - val_accuracy: 0.8121 - val_loss: 0.3868\n",
      "Epoch 7/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.8072 - loss: 0.4124 - val_accuracy: 0.8250 - val_loss: 0.3828\n",
      "Epoch 8/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - accuracy: 0.8159 - loss: 0.3931 - val_accuracy: 0.8224 - val_loss: 0.3793\n",
      "Epoch 9/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.8154 - loss: 0.3866 - val_accuracy: 0.8400 - val_loss: 0.3441\n",
      "Epoch 10/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.8367 - loss: 0.3575 - val_accuracy: 0.8487 - val_loss: 0.3389\n",
      "Training with optimizer=adam, dropout_rate=0.3, batch_size=32, epochs=20\n",
      "Epoch 1/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 53ms/step - accuracy: 0.5700 - loss: 0.6910 - val_accuracy: 0.6200 - val_loss: 0.6738\n",
      "Epoch 2/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - accuracy: 0.6183 - loss: 0.6619 - val_accuracy: 0.6773 - val_loss: 0.6085\n",
      "Epoch 3/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.6820 - loss: 0.5991 - val_accuracy: 0.7506 - val_loss: 0.5147\n",
      "Epoch 4/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.7306 - loss: 0.5349 - val_accuracy: 0.7424 - val_loss: 0.5131\n",
      "Epoch 5/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.7537 - loss: 0.5097 - val_accuracy: 0.7641 - val_loss: 0.4671\n",
      "Epoch 6/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.7796 - loss: 0.4743 - val_accuracy: 0.7842 - val_loss: 0.4395\n",
      "Epoch 7/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.7820 - loss: 0.4513 - val_accuracy: 0.8080 - val_loss: 0.4067\n",
      "Epoch 8/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 65ms/step - accuracy: 0.7929 - loss: 0.4244 - val_accuracy: 0.8209 - val_loss: 0.3777\n",
      "Epoch 9/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 70ms/step - accuracy: 0.8161 - loss: 0.3947 - val_accuracy: 0.8394 - val_loss: 0.3627\n",
      "Epoch 10/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 56ms/step - accuracy: 0.8272 - loss: 0.3771 - val_accuracy: 0.8183 - val_loss: 0.3890\n",
      "Epoch 11/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 0.8292 - loss: 0.3693 - val_accuracy: 0.8503 - val_loss: 0.3386\n",
      "Epoch 12/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - accuracy: 0.8359 - loss: 0.3556 - val_accuracy: 0.8451 - val_loss: 0.3370\n",
      "Epoch 13/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 47ms/step - accuracy: 0.8522 - loss: 0.3279 - val_accuracy: 0.8441 - val_loss: 0.3301\n",
      "Epoch 14/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.8528 - loss: 0.3243 - val_accuracy: 0.8544 - val_loss: 0.3213\n",
      "Epoch 15/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.8492 - loss: 0.3259 - val_accuracy: 0.8632 - val_loss: 0.3130\n",
      "Epoch 16/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - accuracy: 0.8605 - loss: 0.3009 - val_accuracy: 0.8637 - val_loss: 0.3141\n",
      "Epoch 17/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.8704 - loss: 0.2963 - val_accuracy: 0.8601 - val_loss: 0.3116\n",
      "Epoch 18/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.8816 - loss: 0.2776 - val_accuracy: 0.8699 - val_loss: 0.3098\n",
      "Epoch 19/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.8722 - loss: 0.2786 - val_accuracy: 0.8622 - val_loss: 0.3082\n",
      "Epoch 20/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.8881 - loss: 0.2611 - val_accuracy: 0.8668 - val_loss: 0.3112\n",
      "Training with optimizer=adam, dropout_rate=0.3, batch_size=64, epochs=10\n",
      "Epoch 1/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 83ms/step - accuracy: 0.5904 - loss: 0.6772 - val_accuracy: 0.5978 - val_loss: 0.6749\n",
      "Epoch 2/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - accuracy: 0.6362 - loss: 0.6385 - val_accuracy: 0.6913 - val_loss: 0.5556\n",
      "Epoch 3/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - accuracy: 0.7294 - loss: 0.5332 - val_accuracy: 0.7512 - val_loss: 0.5020\n",
      "Epoch 4/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - accuracy: 0.7608 - loss: 0.4867 - val_accuracy: 0.7956 - val_loss: 0.4474\n",
      "Epoch 5/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - accuracy: 0.7863 - loss: 0.4477 - val_accuracy: 0.8260 - val_loss: 0.4019\n",
      "Epoch 6/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.7967 - loss: 0.4233 - val_accuracy: 0.8250 - val_loss: 0.3843\n",
      "Epoch 7/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - accuracy: 0.8116 - loss: 0.4080 - val_accuracy: 0.8327 - val_loss: 0.3749\n",
      "Epoch 8/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - accuracy: 0.8155 - loss: 0.3887 - val_accuracy: 0.8425 - val_loss: 0.3644\n",
      "Epoch 9/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - accuracy: 0.8275 - loss: 0.3765 - val_accuracy: 0.8451 - val_loss: 0.3405\n",
      "Epoch 10/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - accuracy: 0.8357 - loss: 0.3507 - val_accuracy: 0.8565 - val_loss: 0.3253\n",
      "Training with optimizer=adam, dropout_rate=0.3, batch_size=64, epochs=20\n",
      "Epoch 1/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 87ms/step - accuracy: 0.5735 - loss: 0.6928 - val_accuracy: 0.5911 - val_loss: 0.6817\n",
      "Epoch 2/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 91ms/step - accuracy: 0.5901 - loss: 0.6706 - val_accuracy: 0.6618 - val_loss: 0.6058\n",
      "Epoch 3/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 89ms/step - accuracy: 0.6580 - loss: 0.6232 - val_accuracy: 0.7315 - val_loss: 0.5479\n",
      "Epoch 4/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - accuracy: 0.7237 - loss: 0.5506 - val_accuracy: 0.7615 - val_loss: 0.4893\n",
      "Epoch 5/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.7376 - loss: 0.5148 - val_accuracy: 0.7687 - val_loss: 0.4753\n",
      "Epoch 6/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.7763 - loss: 0.4699 - val_accuracy: 0.7790 - val_loss: 0.4395\n",
      "Epoch 7/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.7808 - loss: 0.4424 - val_accuracy: 0.8141 - val_loss: 0.4030\n",
      "Epoch 8/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - accuracy: 0.8028 - loss: 0.4205 - val_accuracy: 0.8136 - val_loss: 0.4015\n",
      "Epoch 9/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.8197 - loss: 0.3929 - val_accuracy: 0.8281 - val_loss: 0.3840\n",
      "Epoch 10/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - accuracy: 0.8263 - loss: 0.3800 - val_accuracy: 0.8389 - val_loss: 0.3580\n",
      "Epoch 11/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - accuracy: 0.8268 - loss: 0.3667 - val_accuracy: 0.8394 - val_loss: 0.3562\n",
      "Epoch 12/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 78ms/step - accuracy: 0.8285 - loss: 0.3634 - val_accuracy: 0.8518 - val_loss: 0.3404\n",
      "Epoch 13/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - accuracy: 0.8494 - loss: 0.3296 - val_accuracy: 0.8544 - val_loss: 0.3497\n",
      "Epoch 14/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.8519 - loss: 0.3317 - val_accuracy: 0.8493 - val_loss: 0.3371\n",
      "Epoch 15/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - accuracy: 0.8512 - loss: 0.3254 - val_accuracy: 0.8601 - val_loss: 0.3263\n",
      "Epoch 16/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.8611 - loss: 0.3023 - val_accuracy: 0.8560 - val_loss: 0.3265\n",
      "Epoch 17/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.8794 - loss: 0.2779 - val_accuracy: 0.8554 - val_loss: 0.3273\n",
      "Epoch 18/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - accuracy: 0.8720 - loss: 0.2877 - val_accuracy: 0.8632 - val_loss: 0.3137\n",
      "Epoch 19/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - accuracy: 0.8807 - loss: 0.2774 - val_accuracy: 0.8647 - val_loss: 0.3172\n",
      "Epoch 20/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - accuracy: 0.8897 - loss: 0.2524 - val_accuracy: 0.8487 - val_loss: 0.3257\n",
      "Training with optimizer=adam, dropout_rate=0.4, batch_size=32, epochs=10\n",
      "Epoch 1/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 44ms/step - accuracy: 0.5836 - loss: 0.6810 - val_accuracy: 0.6020 - val_loss: 0.6586\n",
      "Epoch 2/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.6480 - loss: 0.6296 - val_accuracy: 0.7068 - val_loss: 0.5555\n",
      "Epoch 3/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.7066 - loss: 0.5530 - val_accuracy: 0.7868 - val_loss: 0.4676\n",
      "Epoch 4/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7527 - loss: 0.4898 - val_accuracy: 0.7976 - val_loss: 0.4433\n",
      "Epoch 5/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.7816 - loss: 0.4557 - val_accuracy: 0.8141 - val_loss: 0.4066\n",
      "Epoch 6/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.7813 - loss: 0.4500 - val_accuracy: 0.8255 - val_loss: 0.3878\n",
      "Epoch 7/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.7913 - loss: 0.4274 - val_accuracy: 0.8451 - val_loss: 0.3641\n",
      "Epoch 8/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.8197 - loss: 0.3976 - val_accuracy: 0.8260 - val_loss: 0.4032\n",
      "Epoch 9/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.8062 - loss: 0.4035 - val_accuracy: 0.8462 - val_loss: 0.3549\n",
      "Epoch 10/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.8135 - loss: 0.3986 - val_accuracy: 0.8534 - val_loss: 0.3352\n",
      "Training with optimizer=adam, dropout_rate=0.4, batch_size=32, epochs=20\n",
      "Epoch 1/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 45ms/step - accuracy: 0.5857 - loss: 0.6789 - val_accuracy: 0.5978 - val_loss: 0.6668\n",
      "Epoch 2/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.6088 - loss: 0.6655 - val_accuracy: 0.6840 - val_loss: 0.6052\n",
      "Epoch 3/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6832 - loss: 0.5943 - val_accuracy: 0.7439 - val_loss: 0.5400\n",
      "Epoch 4/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7058 - loss: 0.5532 - val_accuracy: 0.7734 - val_loss: 0.4964\n",
      "Epoch 5/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7456 - loss: 0.5119 - val_accuracy: 0.7785 - val_loss: 0.4542\n",
      "Epoch 6/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7526 - loss: 0.4935 - val_accuracy: 0.7904 - val_loss: 0.4463\n",
      "Epoch 7/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7863 - loss: 0.4525 - val_accuracy: 0.8203 - val_loss: 0.4074\n",
      "Epoch 8/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7865 - loss: 0.4443 - val_accuracy: 0.8105 - val_loss: 0.4040\n",
      "Epoch 9/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8050 - loss: 0.4216 - val_accuracy: 0.8296 - val_loss: 0.3841\n",
      "Epoch 10/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.8066 - loss: 0.4023 - val_accuracy: 0.8322 - val_loss: 0.3793\n",
      "Epoch 11/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.8112 - loss: 0.3966 - val_accuracy: 0.8369 - val_loss: 0.3592\n",
      "Epoch 12/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.8151 - loss: 0.3917 - val_accuracy: 0.8467 - val_loss: 0.3470\n",
      "Epoch 13/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.8340 - loss: 0.3685 - val_accuracy: 0.8389 - val_loss: 0.3545\n",
      "Epoch 14/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.8249 - loss: 0.3870 - val_accuracy: 0.8498 - val_loss: 0.3375\n",
      "Epoch 15/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8233 - loss: 0.3654 - val_accuracy: 0.8596 - val_loss: 0.3344\n",
      "Epoch 16/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8342 - loss: 0.3546 - val_accuracy: 0.8585 - val_loss: 0.3298\n",
      "Epoch 17/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8466 - loss: 0.3389 - val_accuracy: 0.8606 - val_loss: 0.3246\n",
      "Epoch 18/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8493 - loss: 0.3399 - val_accuracy: 0.8684 - val_loss: 0.3213\n",
      "Epoch 19/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8394 - loss: 0.3473 - val_accuracy: 0.8565 - val_loss: 0.3255\n",
      "Epoch 20/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8555 - loss: 0.3326 - val_accuracy: 0.8596 - val_loss: 0.3152\n",
      "Training with optimizer=adam, dropout_rate=0.4, batch_size=64, epochs=10\n",
      "Epoch 1/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 81ms/step - accuracy: 0.5668 - loss: 0.6967 - val_accuracy: 0.5911 - val_loss: 0.6823\n",
      "Epoch 2/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 78ms/step - accuracy: 0.5959 - loss: 0.6621 - val_accuracy: 0.6908 - val_loss: 0.5967\n",
      "Epoch 3/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - accuracy: 0.6785 - loss: 0.6023 - val_accuracy: 0.7377 - val_loss: 0.5400\n",
      "Epoch 4/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - accuracy: 0.7275 - loss: 0.5323 - val_accuracy: 0.7672 - val_loss: 0.4682\n",
      "Epoch 5/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.7496 - loss: 0.5022 - val_accuracy: 0.7945 - val_loss: 0.4442\n",
      "Epoch 6/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - accuracy: 0.7683 - loss: 0.4734 - val_accuracy: 0.7801 - val_loss: 0.4551\n",
      "Epoch 7/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - accuracy: 0.7816 - loss: 0.4568 - val_accuracy: 0.8116 - val_loss: 0.4051\n",
      "Epoch 8/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - accuracy: 0.7883 - loss: 0.4347 - val_accuracy: 0.8188 - val_loss: 0.3920\n",
      "Epoch 9/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - accuracy: 0.8024 - loss: 0.4192 - val_accuracy: 0.8307 - val_loss: 0.3770\n",
      "Epoch 10/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.8042 - loss: 0.4071 - val_accuracy: 0.8353 - val_loss: 0.3655\n",
      "Training with optimizer=adam, dropout_rate=0.4, batch_size=64, epochs=20\n",
      "Epoch 1/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 113ms/step - accuracy: 0.5798 - loss: 0.6818 - val_accuracy: 0.5911 - val_loss: 0.6790\n",
      "Epoch 2/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 91ms/step - accuracy: 0.5966 - loss: 0.6657 - val_accuracy: 0.6933 - val_loss: 0.6070\n",
      "Epoch 3/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 92ms/step - accuracy: 0.6746 - loss: 0.5967 - val_accuracy: 0.7284 - val_loss: 0.5527\n",
      "Epoch 4/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 89ms/step - accuracy: 0.7222 - loss: 0.5323 - val_accuracy: 0.7568 - val_loss: 0.5081\n",
      "Epoch 5/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - accuracy: 0.7546 - loss: 0.4915 - val_accuracy: 0.7981 - val_loss: 0.4394\n",
      "Epoch 6/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - accuracy: 0.7739 - loss: 0.4707 - val_accuracy: 0.8095 - val_loss: 0.4153\n",
      "Epoch 7/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 98ms/step - accuracy: 0.7869 - loss: 0.4419 - val_accuracy: 0.8147 - val_loss: 0.4029\n",
      "Epoch 8/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 90ms/step - accuracy: 0.7960 - loss: 0.4308 - val_accuracy: 0.8209 - val_loss: 0.3900\n",
      "Epoch 9/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 93ms/step - accuracy: 0.7982 - loss: 0.4164 - val_accuracy: 0.8446 - val_loss: 0.3756\n",
      "Epoch 10/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 91ms/step - accuracy: 0.8129 - loss: 0.4003 - val_accuracy: 0.8415 - val_loss: 0.3667\n",
      "Epoch 11/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 94ms/step - accuracy: 0.8250 - loss: 0.3814 - val_accuracy: 0.8462 - val_loss: 0.3497\n",
      "Epoch 12/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - accuracy: 0.8282 - loss: 0.3756 - val_accuracy: 0.8554 - val_loss: 0.3331\n",
      "Epoch 13/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 96ms/step - accuracy: 0.8262 - loss: 0.3670 - val_accuracy: 0.8487 - val_loss: 0.3437\n",
      "Epoch 14/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 105ms/step - accuracy: 0.8477 - loss: 0.3378 - val_accuracy: 0.8611 - val_loss: 0.3293\n",
      "Epoch 15/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 100ms/step - accuracy: 0.8316 - loss: 0.3584 - val_accuracy: 0.8622 - val_loss: 0.3310\n",
      "Epoch 16/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 107ms/step - accuracy: 0.8565 - loss: 0.3251 - val_accuracy: 0.8637 - val_loss: 0.3219\n",
      "Epoch 17/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 99ms/step - accuracy: 0.8505 - loss: 0.3310 - val_accuracy: 0.8699 - val_loss: 0.3110\n",
      "Epoch 18/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 117ms/step - accuracy: 0.8606 - loss: 0.3125 - val_accuracy: 0.8689 - val_loss: 0.3123\n",
      "Epoch 19/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 105ms/step - accuracy: 0.8560 - loss: 0.3163 - val_accuracy: 0.8771 - val_loss: 0.3015\n",
      "Epoch 20/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - accuracy: 0.8620 - loss: 0.3153 - val_accuracy: 0.8658 - val_loss: 0.3130\n",
      "Training with optimizer=adam, dropout_rate=0.5, batch_size=32, epochs=10\n",
      "Epoch 1/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - accuracy: 0.5732 - loss: 0.6890 - val_accuracy: 0.5911 - val_loss: 0.6777\n",
      "Epoch 2/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 0.5877 - loss: 0.6747 - val_accuracy: 0.6386 - val_loss: 0.6528\n",
      "Epoch 3/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 0.6210 - loss: 0.6462 - val_accuracy: 0.6706 - val_loss: 0.5948\n",
      "Epoch 4/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 0.6725 - loss: 0.5950 - val_accuracy: 0.7119 - val_loss: 0.5906\n",
      "Epoch 5/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.7232 - loss: 0.5406 - val_accuracy: 0.7759 - val_loss: 0.4580\n",
      "Epoch 6/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.7296 - loss: 0.5203 - val_accuracy: 0.7976 - val_loss: 0.4531\n",
      "Epoch 7/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.7648 - loss: 0.4945 - val_accuracy: 0.7992 - val_loss: 0.4351\n",
      "Epoch 8/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.7698 - loss: 0.4667 - val_accuracy: 0.8100 - val_loss: 0.4149\n",
      "Epoch 9/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.7713 - loss: 0.4649 - val_accuracy: 0.8147 - val_loss: 0.3967\n",
      "Epoch 10/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.7820 - loss: 0.4610 - val_accuracy: 0.8250 - val_loss: 0.3771\n",
      "Training with optimizer=adam, dropout_rate=0.5, batch_size=32, epochs=20\n",
      "Epoch 1/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 44ms/step - accuracy: 0.5870 - loss: 0.6849 - val_accuracy: 0.5911 - val_loss: 0.6798\n",
      "Epoch 2/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.5950 - loss: 0.6716 - val_accuracy: 0.5911 - val_loss: 0.6789\n",
      "Epoch 3/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.5999 - loss: 0.6566 - val_accuracy: 0.7052 - val_loss: 0.6124\n",
      "Epoch 4/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.6701 - loss: 0.5890 - val_accuracy: 0.7326 - val_loss: 0.5175\n",
      "Epoch 5/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.7131 - loss: 0.5473 - val_accuracy: 0.7734 - val_loss: 0.4921\n",
      "Epoch 6/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.7340 - loss: 0.5172 - val_accuracy: 0.7966 - val_loss: 0.4512\n",
      "Epoch 7/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - accuracy: 0.7574 - loss: 0.4941 - val_accuracy: 0.8018 - val_loss: 0.4312\n",
      "Epoch 8/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 47ms/step - accuracy: 0.7714 - loss: 0.4793 - val_accuracy: 0.8090 - val_loss: 0.4256\n",
      "Epoch 9/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.7756 - loss: 0.4690 - val_accuracy: 0.8219 - val_loss: 0.3990\n",
      "Epoch 10/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.7748 - loss: 0.4515 - val_accuracy: 0.8110 - val_loss: 0.3967\n",
      "Epoch 11/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 0.7860 - loss: 0.4402 - val_accuracy: 0.8250 - val_loss: 0.3848\n",
      "Epoch 12/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - accuracy: 0.7966 - loss: 0.4297 - val_accuracy: 0.8369 - val_loss: 0.3682\n",
      "Epoch 13/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - accuracy: 0.8065 - loss: 0.4098 - val_accuracy: 0.8420 - val_loss: 0.3666\n",
      "Epoch 14/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 51ms/step - accuracy: 0.7924 - loss: 0.4245 - val_accuracy: 0.8291 - val_loss: 0.3811\n",
      "Epoch 15/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.7962 - loss: 0.4212 - val_accuracy: 0.8436 - val_loss: 0.3593\n",
      "Epoch 16/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.8090 - loss: 0.4050 - val_accuracy: 0.8410 - val_loss: 0.3634\n",
      "Epoch 17/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.8118 - loss: 0.4029 - val_accuracy: 0.8420 - val_loss: 0.3576\n",
      "Epoch 18/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.8144 - loss: 0.3872 - val_accuracy: 0.8539 - val_loss: 0.3524\n",
      "Epoch 19/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.8175 - loss: 0.3884 - val_accuracy: 0.8498 - val_loss: 0.3400\n",
      "Epoch 20/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.8190 - loss: 0.3848 - val_accuracy: 0.8462 - val_loss: 0.3455\n",
      "Training with optimizer=adam, dropout_rate=0.5, batch_size=64, epochs=10\n",
      "Epoch 1/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 86ms/step - accuracy: 0.5741 - loss: 0.6958 - val_accuracy: 0.5911 - val_loss: 0.6919\n",
      "Epoch 2/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 93ms/step - accuracy: 0.5792 - loss: 0.6754 - val_accuracy: 0.5911 - val_loss: 0.6796\n",
      "Epoch 3/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 99ms/step - accuracy: 0.5928 - loss: 0.6687 - val_accuracy: 0.6541 - val_loss: 0.6521\n",
      "Epoch 4/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 88ms/step - accuracy: 0.6389 - loss: 0.6407 - val_accuracy: 0.6877 - val_loss: 0.6092\n",
      "Epoch 5/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 92ms/step - accuracy: 0.6751 - loss: 0.6033 - val_accuracy: 0.7057 - val_loss: 0.5579\n",
      "Epoch 6/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 88ms/step - accuracy: 0.6981 - loss: 0.5713 - val_accuracy: 0.7264 - val_loss: 0.5328\n",
      "Epoch 7/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - accuracy: 0.7116 - loss: 0.5476 - val_accuracy: 0.7543 - val_loss: 0.5062\n",
      "Epoch 8/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - accuracy: 0.7210 - loss: 0.5358 - val_accuracy: 0.7728 - val_loss: 0.4743\n",
      "Epoch 9/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.7434 - loss: 0.5104 - val_accuracy: 0.7904 - val_loss: 0.4700\n",
      "Epoch 10/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - accuracy: 0.7665 - loss: 0.4758 - val_accuracy: 0.7956 - val_loss: 0.4594\n",
      "Training with optimizer=adam, dropout_rate=0.5, batch_size=64, epochs=20\n",
      "Epoch 1/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 89ms/step - accuracy: 0.5667 - loss: 0.6933 - val_accuracy: 0.5911 - val_loss: 0.6870\n",
      "Epoch 2/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.5941 - loss: 0.6732 - val_accuracy: 0.5911 - val_loss: 0.6901\n",
      "Epoch 3/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.5973 - loss: 0.6685 - val_accuracy: 0.6412 - val_loss: 0.6644\n",
      "Epoch 4/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - accuracy: 0.6271 - loss: 0.6443 - val_accuracy: 0.6856 - val_loss: 0.6088\n",
      "Epoch 5/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - accuracy: 0.6803 - loss: 0.5991 - val_accuracy: 0.7248 - val_loss: 0.5686\n",
      "Epoch 6/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - accuracy: 0.7062 - loss: 0.5686 - val_accuracy: 0.7331 - val_loss: 0.5243\n",
      "Epoch 7/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 87ms/step - accuracy: 0.7173 - loss: 0.5491 - val_accuracy: 0.7734 - val_loss: 0.4855\n",
      "Epoch 8/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.7455 - loss: 0.5105 - val_accuracy: 0.7584 - val_loss: 0.4754\n",
      "Epoch 9/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - accuracy: 0.7546 - loss: 0.4914 - val_accuracy: 0.8043 - val_loss: 0.4346\n",
      "Epoch 10/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.7715 - loss: 0.4733 - val_accuracy: 0.8250 - val_loss: 0.4242\n",
      "Epoch 11/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - accuracy: 0.7656 - loss: 0.4628 - val_accuracy: 0.7981 - val_loss: 0.4334\n",
      "Epoch 12/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - accuracy: 0.7783 - loss: 0.4600 - val_accuracy: 0.8271 - val_loss: 0.3964\n",
      "Epoch 13/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 87ms/step - accuracy: 0.7835 - loss: 0.4355 - val_accuracy: 0.8327 - val_loss: 0.3947\n",
      "Epoch 14/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 92ms/step - accuracy: 0.7900 - loss: 0.4297 - val_accuracy: 0.8322 - val_loss: 0.3658\n",
      "Epoch 15/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 88ms/step - accuracy: 0.7931 - loss: 0.4306 - val_accuracy: 0.8286 - val_loss: 0.3757\n",
      "Epoch 16/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 87ms/step - accuracy: 0.8002 - loss: 0.4140 - val_accuracy: 0.8379 - val_loss: 0.3695\n",
      "Epoch 17/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - accuracy: 0.8097 - loss: 0.4040 - val_accuracy: 0.8431 - val_loss: 0.3520\n",
      "Epoch 18/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.8172 - loss: 0.4022 - val_accuracy: 0.8456 - val_loss: 0.3546\n",
      "Epoch 19/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - accuracy: 0.8186 - loss: 0.3830 - val_accuracy: 0.8518 - val_loss: 0.3402\n",
      "Epoch 20/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.8178 - loss: 0.3855 - val_accuracy: 0.8544 - val_loss: 0.3403\n",
      "Training with optimizer=rmsprop, dropout_rate=0.3, batch_size=32, epochs=10\n",
      "Epoch 1/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 45ms/step - accuracy: 0.5762 - loss: 0.6836 - val_accuracy: 0.6944 - val_loss: 0.6425\n",
      "Epoch 2/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.6744 - loss: 0.6002 - val_accuracy: 0.7537 - val_loss: 0.5050\n",
      "Epoch 3/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7430 - loss: 0.5052 - val_accuracy: 0.7950 - val_loss: 0.4302\n",
      "Epoch 4/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7745 - loss: 0.4655 - val_accuracy: 0.7697 - val_loss: 0.4427\n",
      "Epoch 5/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.7915 - loss: 0.4288 - val_accuracy: 0.8229 - val_loss: 0.3816\n",
      "Epoch 6/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8049 - loss: 0.4263 - val_accuracy: 0.8234 - val_loss: 0.3729\n",
      "Epoch 7/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.8145 - loss: 0.3940 - val_accuracy: 0.8307 - val_loss: 0.3766\n",
      "Epoch 8/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8382 - loss: 0.3546 - val_accuracy: 0.8523 - val_loss: 0.3424\n",
      "Epoch 9/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.8368 - loss: 0.3477 - val_accuracy: 0.8374 - val_loss: 0.3498\n",
      "Epoch 10/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8538 - loss: 0.3257 - val_accuracy: 0.8534 - val_loss: 0.3292\n",
      "Training with optimizer=rmsprop, dropout_rate=0.3, batch_size=32, epochs=20\n",
      "Epoch 1/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 41ms/step - accuracy: 0.5905 - loss: 0.6866 - val_accuracy: 0.6236 - val_loss: 0.6181\n",
      "Epoch 2/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6801 - loss: 0.5939 - val_accuracy: 0.7610 - val_loss: 0.5059\n",
      "Epoch 3/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.7579 - loss: 0.4925 - val_accuracy: 0.8012 - val_loss: 0.4270\n",
      "Epoch 4/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.7802 - loss: 0.4478 - val_accuracy: 0.8245 - val_loss: 0.3885\n",
      "Epoch 5/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.8073 - loss: 0.4040 - val_accuracy: 0.8358 - val_loss: 0.3582\n",
      "Epoch 6/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.8139 - loss: 0.3922 - val_accuracy: 0.8379 - val_loss: 0.3445\n",
      "Epoch 7/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8230 - loss: 0.3785 - val_accuracy: 0.8684 - val_loss: 0.3175\n",
      "Epoch 8/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8306 - loss: 0.3558 - val_accuracy: 0.8544 - val_loss: 0.3227\n",
      "Epoch 9/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8429 - loss: 0.3534 - val_accuracy: 0.8591 - val_loss: 0.3186\n",
      "Epoch 10/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8518 - loss: 0.3201 - val_accuracy: 0.8678 - val_loss: 0.3105\n",
      "Epoch 11/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8602 - loss: 0.3172 - val_accuracy: 0.8658 - val_loss: 0.3118\n",
      "Epoch 12/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8578 - loss: 0.3088 - val_accuracy: 0.8482 - val_loss: 0.3352\n",
      "Epoch 13/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.8760 - loss: 0.2879 - val_accuracy: 0.8684 - val_loss: 0.3032\n",
      "Epoch 14/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8648 - loss: 0.2949 - val_accuracy: 0.8668 - val_loss: 0.3147\n",
      "Epoch 15/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8724 - loss: 0.2737 - val_accuracy: 0.8689 - val_loss: 0.3075\n",
      "Epoch 16/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.8827 - loss: 0.2759 - val_accuracy: 0.8560 - val_loss: 0.3357\n",
      "Epoch 17/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.8907 - loss: 0.2578 - val_accuracy: 0.8771 - val_loss: 0.3087\n",
      "Epoch 18/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.8888 - loss: 0.2598 - val_accuracy: 0.8704 - val_loss: 0.3033\n",
      "Epoch 19/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.8947 - loss: 0.2495 - val_accuracy: 0.8792 - val_loss: 0.2951\n",
      "Epoch 20/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8881 - loss: 0.2503 - val_accuracy: 0.8694 - val_loss: 0.3078\n",
      "Training with optimizer=rmsprop, dropout_rate=0.3, batch_size=64, epochs=10\n",
      "Epoch 1/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 83ms/step - accuracy: 0.5732 - loss: 0.7089 - val_accuracy: 0.5911 - val_loss: 0.6736\n",
      "Epoch 2/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 87ms/step - accuracy: 0.5855 - loss: 0.6726 - val_accuracy: 0.7037 - val_loss: 0.6203\n",
      "Epoch 3/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 88ms/step - accuracy: 0.6762 - loss: 0.6032 - val_accuracy: 0.7310 - val_loss: 0.5443\n",
      "Epoch 4/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - accuracy: 0.7230 - loss: 0.5433 - val_accuracy: 0.7775 - val_loss: 0.4839\n",
      "Epoch 5/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - accuracy: 0.7481 - loss: 0.5049 - val_accuracy: 0.7527 - val_loss: 0.4821\n",
      "Epoch 6/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.7661 - loss: 0.4652 - val_accuracy: 0.7904 - val_loss: 0.4327\n",
      "Epoch 7/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.7900 - loss: 0.4406 - val_accuracy: 0.8172 - val_loss: 0.3999\n",
      "Epoch 8/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - accuracy: 0.8130 - loss: 0.4083 - val_accuracy: 0.8353 - val_loss: 0.3664\n",
      "Epoch 9/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - accuracy: 0.8183 - loss: 0.3844 - val_accuracy: 0.8229 - val_loss: 0.3791\n",
      "Epoch 10/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.8201 - loss: 0.3837 - val_accuracy: 0.8498 - val_loss: 0.3479\n",
      "Training with optimizer=rmsprop, dropout_rate=0.3, batch_size=64, epochs=20\n",
      "Epoch 1/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 82ms/step - accuracy: 0.5892 - loss: 0.7028 - val_accuracy: 0.5911 - val_loss: 0.6753\n",
      "Epoch 2/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.5938 - loss: 0.6691 - val_accuracy: 0.6417 - val_loss: 0.6257\n",
      "Epoch 3/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.6540 - loss: 0.6102 - val_accuracy: 0.7021 - val_loss: 0.5530\n",
      "Epoch 4/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.7217 - loss: 0.5519 - val_accuracy: 0.7636 - val_loss: 0.4978\n",
      "Epoch 5/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.7462 - loss: 0.5109 - val_accuracy: 0.7894 - val_loss: 0.4484\n",
      "Epoch 6/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.7671 - loss: 0.4756 - val_accuracy: 0.8064 - val_loss: 0.4188\n",
      "Epoch 7/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - accuracy: 0.7841 - loss: 0.4437 - val_accuracy: 0.8296 - val_loss: 0.3810\n",
      "Epoch 8/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.8010 - loss: 0.4212 - val_accuracy: 0.8281 - val_loss: 0.3846\n",
      "Epoch 9/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.8112 - loss: 0.4003 - val_accuracy: 0.8301 - val_loss: 0.3648\n",
      "Epoch 10/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 87ms/step - accuracy: 0.8210 - loss: 0.3913 - val_accuracy: 0.8301 - val_loss: 0.3603\n",
      "Epoch 11/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.8339 - loss: 0.3646 - val_accuracy: 0.8332 - val_loss: 0.3628\n",
      "Epoch 12/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - accuracy: 0.8471 - loss: 0.3468 - val_accuracy: 0.8554 - val_loss: 0.3305\n",
      "Epoch 13/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 87ms/step - accuracy: 0.8500 - loss: 0.3339 - val_accuracy: 0.8616 - val_loss: 0.3200\n",
      "Epoch 14/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - accuracy: 0.8535 - loss: 0.3257 - val_accuracy: 0.8689 - val_loss: 0.3066\n",
      "Epoch 15/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - accuracy: 0.8521 - loss: 0.3217 - val_accuracy: 0.8420 - val_loss: 0.3304\n",
      "Epoch 16/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - accuracy: 0.8586 - loss: 0.3076 - val_accuracy: 0.8596 - val_loss: 0.3161\n",
      "Epoch 17/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.8657 - loss: 0.2999 - val_accuracy: 0.8704 - val_loss: 0.3055\n",
      "Epoch 18/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - accuracy: 0.8736 - loss: 0.2906 - val_accuracy: 0.8694 - val_loss: 0.2974\n",
      "Epoch 19/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.8690 - loss: 0.2954 - val_accuracy: 0.8694 - val_loss: 0.3090\n",
      "Epoch 20/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.8721 - loss: 0.2790 - val_accuracy: 0.8668 - val_loss: 0.3079\n",
      "Training with optimizer=rmsprop, dropout_rate=0.4, batch_size=32, epochs=10\n",
      "Epoch 1/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 43ms/step - accuracy: 0.5856 - loss: 0.6923 - val_accuracy: 0.5911 - val_loss: 0.6770\n",
      "Epoch 2/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.6011 - loss: 0.6629 - val_accuracy: 0.7006 - val_loss: 0.6025\n",
      "Epoch 3/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6920 - loss: 0.5851 - val_accuracy: 0.7450 - val_loss: 0.5369\n",
      "Epoch 4/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.7248 - loss: 0.5430 - val_accuracy: 0.7765 - val_loss: 0.4730\n",
      "Epoch 5/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7484 - loss: 0.5092 - val_accuracy: 0.7976 - val_loss: 0.4402\n",
      "Epoch 6/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7639 - loss: 0.4778 - val_accuracy: 0.8095 - val_loss: 0.4190\n",
      "Epoch 7/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7808 - loss: 0.4463 - val_accuracy: 0.8157 - val_loss: 0.4039\n",
      "Epoch 8/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.7820 - loss: 0.4390 - val_accuracy: 0.8240 - val_loss: 0.3781\n",
      "Epoch 9/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.7962 - loss: 0.4216 - val_accuracy: 0.8363 - val_loss: 0.3764\n",
      "Epoch 10/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7957 - loss: 0.4159 - val_accuracy: 0.8141 - val_loss: 0.3861\n",
      "Training with optimizer=rmsprop, dropout_rate=0.4, batch_size=32, epochs=20\n",
      "Epoch 1/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 43ms/step - accuracy: 0.5684 - loss: 0.7105 - val_accuracy: 0.5911 - val_loss: 0.6736\n",
      "Epoch 2/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.6156 - loss: 0.6626 - val_accuracy: 0.6737 - val_loss: 0.5924\n",
      "Epoch 3/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.6841 - loss: 0.5902 - val_accuracy: 0.7558 - val_loss: 0.5154\n",
      "Epoch 4/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.7243 - loss: 0.5392 - val_accuracy: 0.7527 - val_loss: 0.4876\n",
      "Epoch 5/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7622 - loss: 0.4899 - val_accuracy: 0.7852 - val_loss: 0.4470\n",
      "Epoch 6/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.7700 - loss: 0.4677 - val_accuracy: 0.8080 - val_loss: 0.4184\n",
      "Epoch 7/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7775 - loss: 0.4442 - val_accuracy: 0.8265 - val_loss: 0.3842\n",
      "Epoch 8/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 62ms/step - accuracy: 0.7956 - loss: 0.4291 - val_accuracy: 0.8281 - val_loss: 0.3890\n",
      "Epoch 9/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.7997 - loss: 0.4188 - val_accuracy: 0.8472 - val_loss: 0.3634\n",
      "Epoch 10/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.8115 - loss: 0.3989 - val_accuracy: 0.8451 - val_loss: 0.3561\n",
      "Epoch 11/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.8102 - loss: 0.3982 - val_accuracy: 0.8503 - val_loss: 0.3470\n",
      "Epoch 12/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.8235 - loss: 0.3864 - val_accuracy: 0.8518 - val_loss: 0.3316\n",
      "Epoch 13/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.8135 - loss: 0.3963 - val_accuracy: 0.8379 - val_loss: 0.3493\n",
      "Epoch 14/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.8337 - loss: 0.3693 - val_accuracy: 0.8446 - val_loss: 0.3523\n",
      "Epoch 15/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.8323 - loss: 0.3575 - val_accuracy: 0.8570 - val_loss: 0.3249\n",
      "Epoch 16/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.8402 - loss: 0.3505 - val_accuracy: 0.8560 - val_loss: 0.3298\n",
      "Epoch 17/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.8397 - loss: 0.3527 - val_accuracy: 0.8482 - val_loss: 0.3463\n",
      "Epoch 18/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.8411 - loss: 0.3468 - val_accuracy: 0.8554 - val_loss: 0.3438\n",
      "Epoch 19/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.8456 - loss: 0.3441 - val_accuracy: 0.8668 - val_loss: 0.3313\n",
      "Epoch 20/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - accuracy: 0.8410 - loss: 0.3529 - val_accuracy: 0.8668 - val_loss: 0.3141\n",
      "Training with optimizer=rmsprop, dropout_rate=0.4, batch_size=64, epochs=10\n",
      "Epoch 1/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 91ms/step - accuracy: 0.5772 - loss: 0.6896 - val_accuracy: 0.5911 - val_loss: 0.6876\n",
      "Epoch 2/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.6038 - loss: 0.6636 - val_accuracy: 0.6902 - val_loss: 0.6391\n",
      "Epoch 3/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.6551 - loss: 0.6238 - val_accuracy: 0.6717 - val_loss: 0.5844\n",
      "Epoch 4/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - accuracy: 0.7002 - loss: 0.5759 - val_accuracy: 0.7362 - val_loss: 0.5160\n",
      "Epoch 5/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.7361 - loss: 0.5247 - val_accuracy: 0.7646 - val_loss: 0.4833\n",
      "Epoch 6/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - accuracy: 0.7577 - loss: 0.4891 - val_accuracy: 0.8038 - val_loss: 0.4375\n",
      "Epoch 7/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - accuracy: 0.7601 - loss: 0.4707 - val_accuracy: 0.7971 - val_loss: 0.4295\n",
      "Epoch 8/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.7836 - loss: 0.4348 - val_accuracy: 0.8183 - val_loss: 0.3993\n",
      "Epoch 9/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.7964 - loss: 0.4297 - val_accuracy: 0.8234 - val_loss: 0.3872\n",
      "Epoch 10/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.7951 - loss: 0.4234 - val_accuracy: 0.8054 - val_loss: 0.4362\n",
      "Training with optimizer=rmsprop, dropout_rate=0.4, batch_size=64, epochs=20\n",
      "Epoch 1/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 90ms/step - accuracy: 0.5815 - loss: 0.7088 - val_accuracy: 0.5942 - val_loss: 0.6865\n",
      "Epoch 2/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - accuracy: 0.6064 - loss: 0.6673 - val_accuracy: 0.6696 - val_loss: 0.6368\n",
      "Epoch 3/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - accuracy: 0.6631 - loss: 0.6169 - val_accuracy: 0.7001 - val_loss: 0.5557\n",
      "Epoch 4/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - accuracy: 0.7044 - loss: 0.5673 - val_accuracy: 0.7321 - val_loss: 0.5123\n",
      "Epoch 5/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - accuracy: 0.7338 - loss: 0.5180 - val_accuracy: 0.7899 - val_loss: 0.4494\n",
      "Epoch 6/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - accuracy: 0.7579 - loss: 0.4820 - val_accuracy: 0.7981 - val_loss: 0.4240\n",
      "Epoch 7/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - accuracy: 0.7793 - loss: 0.4546 - val_accuracy: 0.7739 - val_loss: 0.4483\n",
      "Epoch 8/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - accuracy: 0.7834 - loss: 0.4463 - val_accuracy: 0.8203 - val_loss: 0.3845\n",
      "Epoch 9/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 89ms/step - accuracy: 0.8040 - loss: 0.4186 - val_accuracy: 0.8224 - val_loss: 0.3712\n",
      "Epoch 10/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.8078 - loss: 0.4082 - val_accuracy: 0.8301 - val_loss: 0.3725\n",
      "Epoch 11/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - accuracy: 0.8141 - loss: 0.3954 - val_accuracy: 0.8400 - val_loss: 0.3506\n",
      "Epoch 12/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - accuracy: 0.8298 - loss: 0.3684 - val_accuracy: 0.8596 - val_loss: 0.3385\n",
      "Epoch 13/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - accuracy: 0.8245 - loss: 0.3800 - val_accuracy: 0.8425 - val_loss: 0.3449\n",
      "Epoch 14/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.8349 - loss: 0.3614 - val_accuracy: 0.8606 - val_loss: 0.3302\n",
      "Epoch 15/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - accuracy: 0.8436 - loss: 0.3495 - val_accuracy: 0.8591 - val_loss: 0.3292\n",
      "Epoch 16/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.8446 - loss: 0.3465 - val_accuracy: 0.8575 - val_loss: 0.3114\n",
      "Epoch 17/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.8429 - loss: 0.3495 - val_accuracy: 0.8606 - val_loss: 0.3142\n",
      "Epoch 18/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - accuracy: 0.8570 - loss: 0.3347 - val_accuracy: 0.8580 - val_loss: 0.3271\n",
      "Epoch 19/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - accuracy: 0.8546 - loss: 0.3203 - val_accuracy: 0.8704 - val_loss: 0.3020\n",
      "Epoch 20/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - accuracy: 0.8595 - loss: 0.3162 - val_accuracy: 0.8570 - val_loss: 0.3185\n",
      "Training with optimizer=rmsprop, dropout_rate=0.5, batch_size=32, epochs=10\n",
      "Epoch 1/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.5939 - loss: 0.7049 - val_accuracy: 0.5911 - val_loss: 0.6908\n",
      "Epoch 2/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.5939 - loss: 0.6768 - val_accuracy: 0.6391 - val_loss: 0.6653\n",
      "Epoch 3/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.6250 - loss: 0.6486 - val_accuracy: 0.6990 - val_loss: 0.5679\n",
      "Epoch 4/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.6962 - loss: 0.5833 - val_accuracy: 0.7445 - val_loss: 0.5228\n",
      "Epoch 5/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.7187 - loss: 0.5449 - val_accuracy: 0.7827 - val_loss: 0.4847\n",
      "Epoch 6/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7348 - loss: 0.5152 - val_accuracy: 0.7940 - val_loss: 0.4369\n",
      "Epoch 7/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.7497 - loss: 0.4953 - val_accuracy: 0.7904 - val_loss: 0.4754\n",
      "Epoch 8/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.7614 - loss: 0.4836 - val_accuracy: 0.7878 - val_loss: 0.4313\n",
      "Epoch 9/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.7747 - loss: 0.4721 - val_accuracy: 0.8245 - val_loss: 0.3910\n",
      "Epoch 10/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.7889 - loss: 0.4452 - val_accuracy: 0.8343 - val_loss: 0.4075\n",
      "Training with optimizer=rmsprop, dropout_rate=0.5, batch_size=32, epochs=20\n",
      "Epoch 1/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 43ms/step - accuracy: 0.5908 - loss: 0.6904 - val_accuracy: 0.5911 - val_loss: 0.6841\n",
      "Epoch 2/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.5887 - loss: 0.6751 - val_accuracy: 0.5911 - val_loss: 0.6484\n",
      "Epoch 3/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.6290 - loss: 0.6386 - val_accuracy: 0.7073 - val_loss: 0.5718\n",
      "Epoch 4/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.6879 - loss: 0.5880 - val_accuracy: 0.7579 - val_loss: 0.5182\n",
      "Epoch 5/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7204 - loss: 0.5469 - val_accuracy: 0.7589 - val_loss: 0.5056\n",
      "Epoch 6/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.7291 - loss: 0.5264 - val_accuracy: 0.7883 - val_loss: 0.4453\n",
      "Epoch 7/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.7470 - loss: 0.4936 - val_accuracy: 0.7641 - val_loss: 0.4716\n",
      "Epoch 8/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.7638 - loss: 0.4755 - val_accuracy: 0.8074 - val_loss: 0.4460\n",
      "Epoch 9/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.7654 - loss: 0.4720 - val_accuracy: 0.8110 - val_loss: 0.4236\n",
      "Epoch 10/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.7829 - loss: 0.4544 - val_accuracy: 0.8240 - val_loss: 0.4363\n",
      "Epoch 11/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.7855 - loss: 0.4506 - val_accuracy: 0.8224 - val_loss: 0.3929\n",
      "Epoch 12/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.7870 - loss: 0.4424 - val_accuracy: 0.8322 - val_loss: 0.3864\n",
      "Epoch 13/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.7910 - loss: 0.4386 - val_accuracy: 0.8018 - val_loss: 0.4377\n",
      "Epoch 14/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.7911 - loss: 0.4408 - val_accuracy: 0.8095 - val_loss: 0.4053\n",
      "Epoch 15/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 0.8021 - loss: 0.4263 - val_accuracy: 0.8389 - val_loss: 0.3686\n",
      "Epoch 16/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.7944 - loss: 0.4191 - val_accuracy: 0.8384 - val_loss: 0.3933\n",
      "Epoch 17/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.8017 - loss: 0.4169 - val_accuracy: 0.8451 - val_loss: 0.3609\n",
      "Epoch 18/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.8058 - loss: 0.4082 - val_accuracy: 0.8446 - val_loss: 0.3933\n",
      "Epoch 19/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.8085 - loss: 0.4158 - val_accuracy: 0.8498 - val_loss: 0.3762\n",
      "Epoch 20/20\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.8129 - loss: 0.4154 - val_accuracy: 0.8441 - val_loss: 0.3777\n",
      "Training with optimizer=rmsprop, dropout_rate=0.5, batch_size=64, epochs=10\n",
      "Epoch 1/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 79ms/step - accuracy: 0.5816 - loss: 0.6974 - val_accuracy: 0.5911 - val_loss: 0.6827\n",
      "Epoch 2/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 75ms/step - accuracy: 0.5929 - loss: 0.6747 - val_accuracy: 0.5911 - val_loss: 0.6849\n",
      "Epoch 3/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - accuracy: 0.5904 - loss: 0.6695 - val_accuracy: 0.6748 - val_loss: 0.6424\n",
      "Epoch 4/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - accuracy: 0.6493 - loss: 0.6230 - val_accuracy: 0.7140 - val_loss: 0.5569\n",
      "Epoch 5/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - accuracy: 0.6897 - loss: 0.5839 - val_accuracy: 0.7341 - val_loss: 0.5335\n",
      "Epoch 6/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.7205 - loss: 0.5435 - val_accuracy: 0.7682 - val_loss: 0.4873\n",
      "Epoch 7/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - accuracy: 0.7442 - loss: 0.5096 - val_accuracy: 0.7739 - val_loss: 0.4667\n",
      "Epoch 8/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - accuracy: 0.7605 - loss: 0.4908 - val_accuracy: 0.7770 - val_loss: 0.4672\n",
      "Epoch 9/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - accuracy: 0.7613 - loss: 0.4865 - val_accuracy: 0.7558 - val_loss: 0.4981\n",
      "Epoch 10/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - accuracy: 0.7700 - loss: 0.4572 - val_accuracy: 0.7765 - val_loss: 0.4466\n",
      "Training with optimizer=rmsprop, dropout_rate=0.5, batch_size=64, epochs=20\n",
      "Epoch 1/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 77ms/step - accuracy: 0.5712 - loss: 0.6946 - val_accuracy: 0.5911 - val_loss: 0.6884\n",
      "Epoch 2/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - accuracy: 0.5865 - loss: 0.6746 - val_accuracy: 0.6505 - val_loss: 0.6873\n",
      "Epoch 3/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - accuracy: 0.6224 - loss: 0.6548 - val_accuracy: 0.7021 - val_loss: 0.6215\n",
      "Epoch 4/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - accuracy: 0.6691 - loss: 0.6106 - val_accuracy: 0.7238 - val_loss: 0.5639\n",
      "Epoch 5/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - accuracy: 0.7026 - loss: 0.5701 - val_accuracy: 0.7403 - val_loss: 0.5242\n",
      "Epoch 6/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 78ms/step - accuracy: 0.7200 - loss: 0.5388 - val_accuracy: 0.7754 - val_loss: 0.4851\n",
      "Epoch 7/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - accuracy: 0.7511 - loss: 0.5047 - val_accuracy: 0.7821 - val_loss: 0.4624\n",
      "Epoch 8/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - accuracy: 0.7534 - loss: 0.4935 - val_accuracy: 0.7605 - val_loss: 0.4712\n",
      "Epoch 9/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - accuracy: 0.7653 - loss: 0.4727 - val_accuracy: 0.7997 - val_loss: 0.4194\n",
      "Epoch 10/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - accuracy: 0.7771 - loss: 0.4565 - val_accuracy: 0.8178 - val_loss: 0.4035\n",
      "Epoch 11/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - accuracy: 0.7845 - loss: 0.4421 - val_accuracy: 0.8307 - val_loss: 0.3884\n",
      "Epoch 12/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - accuracy: 0.7941 - loss: 0.4312 - val_accuracy: 0.7475 - val_loss: 0.4770\n",
      "Epoch 13/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - accuracy: 0.7886 - loss: 0.4308 - val_accuracy: 0.8472 - val_loss: 0.3595\n",
      "Epoch 14/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 110ms/step - accuracy: 0.8065 - loss: 0.4076 - val_accuracy: 0.8369 - val_loss: 0.3665\n",
      "Epoch 15/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 94ms/step - accuracy: 0.8103 - loss: 0.4079 - val_accuracy: 0.8327 - val_loss: 0.3616\n",
      "Epoch 16/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 98ms/step - accuracy: 0.8071 - loss: 0.4168 - val_accuracy: 0.8312 - val_loss: 0.3638\n",
      "Epoch 17/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 115ms/step - accuracy: 0.8103 - loss: 0.3945 - val_accuracy: 0.8539 - val_loss: 0.3508\n",
      "Epoch 18/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 149ms/step - accuracy: 0.8161 - loss: 0.4004 - val_accuracy: 0.8591 - val_loss: 0.3623\n",
      "Epoch 19/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 95ms/step - accuracy: 0.8173 - loss: 0.3885 - val_accuracy: 0.8482 - val_loss: 0.3844\n",
      "Epoch 20/20\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 97ms/step - accuracy: 0.8263 - loss: 0.3850 - val_accuracy: 0.8493 - val_loss: 0.3479\n",
      "Best Score: 0.8791946172714233\n",
      "Best Params: {'optimizer': 'rmsprop', 'dropout_rate': 0.3, 'batch_size': 32, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "best_score, best_params = cnn_grid_search(X_train, y_train_cat, param_grid)\n",
    "print(f\"Best Score: {best_score}\")\n",
    "print(f\"Best Params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\buque\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 47ms/step - accuracy: 0.5860 - loss: 0.6887 - val_accuracy: 0.6993 - val_loss: 0.5821\n",
      "Epoch 2/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 47ms/step - accuracy: 0.6965 - loss: 0.5754 - val_accuracy: 0.7798 - val_loss: 0.4589\n",
      "Epoch 3/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - accuracy: 0.7591 - loss: 0.4782 - val_accuracy: 0.7947 - val_loss: 0.4390\n",
      "Epoch 4/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 48ms/step - accuracy: 0.7814 - loss: 0.4361 - val_accuracy: 0.8125 - val_loss: 0.3997\n",
      "Epoch 5/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - accuracy: 0.8167 - loss: 0.3911 - val_accuracy: 0.8344 - val_loss: 0.3553\n",
      "Epoch 6/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.8229 - loss: 0.3773 - val_accuracy: 0.8278 - val_loss: 0.3676\n",
      "Epoch 7/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 44ms/step - accuracy: 0.8375 - loss: 0.3593 - val_accuracy: 0.8340 - val_loss: 0.3505\n",
      "Epoch 8/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8408 - loss: 0.3473 - val_accuracy: 0.8542 - val_loss: 0.3215\n",
      "Epoch 9/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.8469 - loss: 0.3298 - val_accuracy: 0.8426 - val_loss: 0.3401\n",
      "Epoch 10/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8630 - loss: 0.3196 - val_accuracy: 0.8612 - val_loss: 0.3164\n",
      "Epoch 11/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.8598 - loss: 0.3246 - val_accuracy: 0.8658 - val_loss: 0.3004\n",
      "Epoch 12/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8646 - loss: 0.3090 - val_accuracy: 0.8695 - val_loss: 0.2944\n",
      "Epoch 13/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.8655 - loss: 0.3075 - val_accuracy: 0.8352 - val_loss: 0.3457\n",
      "Epoch 14/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 44ms/step - accuracy: 0.8750 - loss: 0.2897 - val_accuracy: 0.8728 - val_loss: 0.3030\n",
      "Epoch 15/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 44ms/step - accuracy: 0.8813 - loss: 0.2710 - val_accuracy: 0.8699 - val_loss: 0.2884\n",
      "Epoch 16/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8853 - loss: 0.2632 - val_accuracy: 0.8748 - val_loss: 0.2864\n",
      "Epoch 17/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 44ms/step - accuracy: 0.8864 - loss: 0.2667 - val_accuracy: 0.8525 - val_loss: 0.3270\n",
      "Epoch 18/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 43ms/step - accuracy: 0.8898 - loss: 0.2560 - val_accuracy: 0.8777 - val_loss: 0.2859\n",
      "Epoch 19/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 48ms/step - accuracy: 0.8981 - loss: 0.2410 - val_accuracy: 0.8736 - val_loss: 0.2869\n",
      "Epoch 20/20\n",
      "\u001b[1m303/303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - accuracy: 0.8915 - loss: 0.2598 - val_accuracy: 0.8769 - val_loss: 0.2842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22176b1b650>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the final model with the best hyperparameters\n",
    "best_model = create_cnn_model(optimizer=best_params['optimizer'], dropout_rate=best_params['dropout_rate'])\n",
    "best_model.fit(X_train, y_train_cat, batch_size=best_params['batch_size'], epochs=best_params['epochs'], validation_data=(X_val, y_val_cat), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, test_filenames = fc.load_images_from_folder(test_folder_path, label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = best_model.predict(X_test)\n",
    "predicted_labels = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_img</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7061</th>\n",
       "      <td>7061</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7062</th>\n",
       "      <td>7062</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7063</th>\n",
       "      <td>7063</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7064</th>\n",
       "      <td>7064</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7065</th>\n",
       "      <td>7065</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7066 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_img  label\n",
       "0          0    sad\n",
       "1          1  happy\n",
       "2          2    sad\n",
       "3          3    sad\n",
       "4          4    sad\n",
       "...      ...    ...\n",
       "7061    7061  happy\n",
       "7062    7062  happy\n",
       "7063    7063  happy\n",
       "7064    7064  happy\n",
       "7065    7065  happy\n",
       "\n",
       "[7066 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id_img': list(range(len(test_filenames))),\n",
    "    'label': ['happy' if label == 1 else 'sad' for label in predicted_labels]\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission.drop_duplicates(subset=['id_img'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_img</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7061</th>\n",
       "      <td>7061</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7062</th>\n",
       "      <td>7062</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7063</th>\n",
       "      <td>7063</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7064</th>\n",
       "      <td>7064</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7065</th>\n",
       "      <td>7065</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7066 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_img  label\n",
       "0          0    sad\n",
       "1          1  happy\n",
       "2          2    sad\n",
       "3          3    sad\n",
       "4          4    sad\n",
       "...      ...    ...\n",
       "7061    7061  happy\n",
       "7062    7062  happy\n",
       "7063    7063  happy\n",
       "7064    7064  happy\n",
       "7065    7065  happy\n",
       "\n",
       "[7066 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
